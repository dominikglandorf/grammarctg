{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b887067c-3f82-459f-98ff-14db6836fd1d",
   "metadata": {},
   "source": [
    "# Exp020: Eliciting target grammar skills\n",
    "This is an analysis for relationships between grammar skills that may be exploited for scaffolding learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dacfa615-d1d4-41e7-8627-9ec06df0f039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from '/cluster/home/dglandorf/grammarctg/experiments/../source/helpers.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ['CACHE_DIR'] = os.environ['FAST_CACHE_DIR'].replace(\"%SLURM_JOB_ID%\", os.getenv('SLURM_JOB_ID')) # speed up model loading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "from scipy.stats import norm\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'../source')\n",
    "import data\n",
    "import evaluation\n",
    "import helpers\n",
    "import models\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc3fa3-2f61-40f0-b9b6-0634931cf5e6",
   "metadata": {},
   "source": [
    "Load dialogs and EGP skills that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94e0ad03-031a-4471-8bad-b3b24d750aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs = data.get_dialog_data()\n",
    "turns = helpers.flatten_list_of_lists([d[0] for d in dialogs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0b12c-490f-4e06-9c8f-2c22251ccece",
   "metadata": {},
   "source": [
    "The attention mask defines which other turns are of interest in between-speaker priming. It has lower triangular pattern of attention to every other diagonal up to a certain lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee3c30dd-c1fc-4379-9148-70f63df48076",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_turns = [len(d[0]) for d in dialogs]\n",
    "turn_attention = []\n",
    "n_resp = 2\n",
    "idx = 0\n",
    "for length in n_turns:\n",
    "    for i in range(0, length):\n",
    "        attention = []\n",
    "        for k in range(i, min(length-1, i+(2*n_resp-1)), 2): # only attend to next n response\n",
    "            attention += [idx+k+1]\n",
    "        turn_attention.append(attention)\n",
    "    idx += length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce794aea-ab9b-4a61-a84f-f73f5e9755d2",
   "metadata": {},
   "source": [
    "Let's classify the grammar in all turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae85efd3-e078-4cb1-9d93-20bcb37aa412",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = helpers.get_high_conf_classifiers()\n",
    "classifiers = {nr: models.load_classifier(nr, \"corpus_training\") for nr in skills}\n",
    "classified_file = '../data/turn_classification.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fd563a8-dc86-4def-981f-5b781b1f3233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51e0f723a0b46b9ba8b2faaddc93a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentence tokenization:   0%|          | 0/710640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_turns = int(1e10)\n",
    "sentences = [(idx, sentence) for idx, turn in tqdm(enumerate(turns[:max_turns]), total=len(turns), desc=\"Sentence tokenization\") for sentence in data.sent_tokenize(turn)]\n",
    "indices, sents = [s[0] for s in sentences], [s[1] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b55532-4462-44c8-8ec4-722658992203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96990878197648fd93a5122fd6c214b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentence tokenization:   0%|          | 0/710640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c4bef296264b2ca214649591933cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grammar classification:   0%|          | 0/2371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_inputs = models.bert_tokenizer(sents, return_tensors='pt', max_length=64, padding='max_length', truncation=True)\n",
    "dataset = TensorDataset(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "clf_hits = {nr: [] for nr in skills}\n",
    "for input_ids, attention_mask in tqdm(dataloader, desc=\"Grammar classification\"):    \n",
    "    input_ids, attention_mask = input_ids.to(models.device), attention_mask.to(models.device)\n",
    "    with torch.no_grad():\n",
    "        encoded_inputs = models.bert_encoder(input_ids, attention_mask) # encoding is the same for all classifiers\n",
    "        x = torch.cat(encoded_inputs.hidden_states, dim=-1)\n",
    "        for nr, clf in classifiers.items():\n",
    "            max_values, _ = clf.forward_bert(x, attention_mask)\n",
    "            clf_hits[nr] += (max_values>0.5).cpu().tolist()\n",
    "\n",
    "with open(classified_file, 'wb') as f:\n",
    "    pickle.dump(clf_hits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0ef70a3-4691-4225-ae2a-b3e58e1ad2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(classified_file, 'rb') as f:\n",
    "    clf_hits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a004a-a8ce-4fbf-8671-f6d3a9513c0c",
   "metadata": {},
   "source": [
    "Transform the classification into one big indicator matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db314707-07d7-4e6c-84c1-c963f9ef1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = np.zeros([sum(n_turns), len(skills)], dtype=bool)\n",
    "for idx, (nr, clf_hit) in enumerate(clf_hits.items()):\n",
    "    hit_indices = np.array(indices)[clf_hit]\n",
    "    hits[hit_indices,idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a3bfb60-5650-4199-96d8-da930dfb889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rate = hits.sum(axis=0)/min(max_turns, len(turns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e40e82a5-55bc-459b-bb01-73af60c60dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0002941 , 0.01075509, 0.0044284 , 0.00390352, 0.00116937,\n",
       "       0.00012946, 0.00939013, 0.0122453 , 0.05582433, 0.00396825,\n",
       "       0.00505882, 0.00543876, 0.00704435, 0.00227541, 0.00202071,\n",
       "       0.00289035, 0.0016844 , 0.00027159, 0.00093296, 0.00075144,\n",
       "       0.00412023, 0.00024344, 0.04089835, 0.04483001, 0.01540724,\n",
       "       0.01069177, 0.027077  , 0.01082123, 0.02725571, 0.0012932 ,\n",
       "       0.00024204])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b931455-f768-4920-9f84-ee8db112574f",
   "metadata": {},
   "source": [
    "Check how often we have found a certain skill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c078e4-4f29-4242-92d9-9b7458de4b22",
   "metadata": {},
   "source": [
    "Let's bring them into a format for statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "733490e8-5367-42cd-8be1-cfb4ff37535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = lambda z: (1 - norm.cdf(z)) # two-sided p-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8cdcca38-2db0-4759-abe0-ced631b8e650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "[]\n",
      "58\n",
      "[]\n",
      "59\n",
      "[(619, 0.054986076273560974)]\n",
      "69\n",
      "[]\n",
      "76\n",
      "[(619, 0.08029421429804173)]\n",
      "77\n",
      "[(619, 0.05789316072273429)]\n",
      "616\n",
      "[(618, 0.06825118066620305), (619, 0.050045011648444646), (621, 0.050059317762962956)]\n",
      "618\n",
      "[(616, 0.05910301877374047), (618, 0.09046076773051631), (619, 0.09161875884694422)]\n",
      "619\n",
      "[(619, 0.10942033133975915)]\n",
      "621\n",
      "[(616, 0.13200107371930717), (618, 0.13418241925913368), (619, 0.19478257184029837)]\n",
      "624\n",
      "[(619, 0.12330353432522953)]\n",
      "625\n",
      "[(619, 0.16144102063730528)]\n",
      "628\n",
      "[(619, 0.18890422698684195)]\n",
      "629\n",
      "[(619, 0.11872153470521929)]\n",
      "630\n",
      "[(619, 0.122533173407217)]\n",
      "631\n",
      "[(619, 0.11197503006872904)]\n",
      "634\n",
      "[(619, 0.06873479765482438)]\n",
      "635\n",
      "[(619, 0.08708255346296576), (625, 0.052078297841107415), (1176, 0.06338116502599461)]\n",
      "636\n",
      "[(619, 0.11094346393236357)]\n",
      "1106\n",
      "[]\n",
      "1112\n",
      "[(619, 0.17899021719310104)]\n",
      "1116\n",
      "[(625, 0.12864763029635046), (631, 0.11013729461719923), (1116, 0.09205062393585929), (1175, 0.1312842092115411), (1179, 0.1133801831931909)]\n",
      "1175\n",
      "[]\n",
      "1176\n",
      "[]\n",
      "1177\n",
      "[]\n",
      "1178\n",
      "[]\n",
      "1179\n",
      "[(1179, 0.06077769288314755)]\n",
      "1187\n",
      "[]\n",
      "1192\n",
      "[]\n",
      "1197\n",
      "[]\n",
      "1198\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "p_thres = 1e-3\n",
    "primed = {}\n",
    "for idx, nr in enumerate(skills):\n",
    "    print(nr)\n",
    "    turns_with_skill = set(np.where(hits[:,idx])[0])\n",
    "\n",
    "    antecedent_skills = np.vstack([hits[turn_attention[turn_nr],:].any(axis=0) for turn_nr in turns_with_skill])\n",
    "    other_skills = np.vstack([hits[turn_attention[turn_nr],:].any(axis=0) for turn_nr in range(len(turns)) if turn_nr not in turns_with_skill])\n",
    "\n",
    "    # null hypothesis: p1(chance of target skill after prime)=p2(chance of not target skill after prime)\n",
    "    # in this case: p(base rate of target skill)\n",
    "    p = base_rate[idx]\n",
    "    x1 = antecedent_skills.sum(axis=0)\n",
    "    n1 = len(antecedent_skills)\n",
    "    p1 = x1 / n1\n",
    "    x2 = other_skills.sum(axis=0)\n",
    "    n2 = len(other_skills)\n",
    "    p2 = x2 / n2\n",
    "    \n",
    "    SE = np.sqrt(p * (1 - p) * (1/n1 + 1/n2))\n",
    "    z = (p1 - p2) / SE\n",
    "    p = p_val(z)\n",
    "\n",
    "    indices = np.where(((p1-p2)>0.05))\n",
    "    print(list(zip(np.array(skills)[indices], (p1-p2)[indices])))\n",
    "    primed[nr] = list(np.array(skills)[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43f5b0-d9c6-4c71-ad2e-0b8349751fb9",
   "metadata": {},
   "source": [
    "## Establish causality by simulating an intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b79ae-1cba-41df-a8fd-fb9168fa194a",
   "metadata": {},
   "source": [
    "Now let's use one of them in a random dialog context and simulate the next answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137e4c9b-d084-48f8-92de-0a815295c33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a956e1e28d374aa595f90ff76652d7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7cc02b4b484863b82715d5972df8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5d5bf555214db28eebe4480db344c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f1d566b23c4579bfb3d56d295cb400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252f262a9ae040eea8b75b085cb24fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ead1c5c223243aba7f4d3c0370bae36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e0650d258e42e688c75c2132f2abf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af156c56b0c74107861284bb70c3dda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a57224093514647affbf1faecb39e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d205243c224e6cb036e3ba808319e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0871791dc124d93abde6f5d023730ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858df0af5b0745a3bb45540d0166a10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = models.load_generator(\"meta-llama/Meta-Llama-3-8B-Instruct\")#\"/cluster/home/dglandorf/models/llama-FT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f72a380-f35e-4862-be52-62c83af701e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_json(\"../data/task1/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f0cf9-b4dc-4a35-b271-502de1908da6",
   "metadata": {},
   "source": [
    "Sample random contexts and override the constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4f0acf22-6f05-49db-98ec-3178eb884f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "nr = 621\n",
    "cases = testset.sample(n)\n",
    "cases['constraints']=[[nr]] * n\n",
    "cases = cases.apply(lambda x: helpers.get_generation_prompt(x, tokenizer.apply_chat_template, system_msg=True), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2f010-06f9-42e0-bb66-5b8f4582f475",
   "metadata": {},
   "source": [
    "Generate constrained answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b922820b-2bfa-434c-888e-a4b87637c965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:57<00:00,  7.17s/it]\n"
     ]
    }
   ],
   "source": [
    "cases['response'] = models.generate(model, tokenizer, list(cases['prompt']), verbose=False, do_sample=False, eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")], batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e0253-5bcf-4aff-847e-a8ea703dfb32",
   "metadata": {},
   "source": [
    "Append the answers to the context that fulfill the constraint and create prompts for an unconstrained answer to simulate the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "34738efa-b5d3-4e6a-8f04-2be9d7586019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(cases))\n",
    "cases = cases[(models.probe_model(classifiers[nr], list(cases['response']))[0] > 0.5).numpy()]\n",
    "print(len(cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "76d8e174-7df0-454e-82e6-dadaf8c71b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases['context'] = cases.apply(lambda x: x['context'] + [x['response'].replace(\"A: \", \"\")], axis=1)\n",
    "cases = cases.apply(lambda x: helpers.get_generation_prompt(x, tokenizer.apply_chat_template, system_msg=True, unconstrained=True), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a49779-7fdc-47ad-98ed-c55c543e6d7d",
   "metadata": {},
   "source": [
    "Generate next response again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f2f4038d-5b13-4c39-a0e2-284a44def3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [01:23<00:00, 10.45s/it]\n"
     ]
    }
   ],
   "source": [
    "cases['response'] = models.generate(model, tokenizer, list(cases['prompt']), verbose=False, do_sample=False, eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3755ded1-d3ee-482c-b2df-760087132a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases['context'] = cases.apply(lambda x: x['context'] + [x['response'].replace(\"A: \", \"\")], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "51405747-56d0-465b-a92b-dc105987bd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hmm interesting. World War 2 sounds like a periodic action film. What is the genre of the film?',\n",
       " 'The genre is drama, and it is a biopic because it details biographical information about Alan Turing.',\n",
       " 'Wow. Must be quite fun. I have not heard of the director but I have heard of Benedict Cumberbatch. I like his films. What did you enjoy the most about this movie?',\n",
       " \"Actually, what I enjoyed most was Benedict Cumberbatch's performance. He was excellent!\",\n",
       " 'Would you like to watch another film about a historical figure, like Churchill or Einstein?',\n",
       " 'Would you like to watch another film about a historical figure, like Churchill or Einstein?',\n",
       " \"That's a great idea! I'd love to watch a film about Churchill. I've always been fascinated by his leadership during World War 2.\"]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases['context'].sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91dbdb-19e1-4838-9870-f7ec8bbb270b",
   "metadata": {},
   "source": [
    "Test for target structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "08cc1623-4e34-47d2-9436-5b3d3287021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n",
      "tensor(0.)\n",
      "618\n",
      "tensor(0.)\n",
      "619\n",
      "tensor(0.4688)\n"
     ]
    }
   ],
   "source": [
    "for nr in primed[nr]:\n",
    "    print(nr)\n",
    "    print((models.probe_model(classifiers[nr], list(cases['response']))[0]>0.5).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e49bb30-d968-450d-961a-d3131e5ca962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two number 3s, please.',\n",
       " 'All right. What would you like to drink?',\n",
       " 'Diet Coke.',\n",
       " 'Regular or large?',\n",
       " 'If I had known the party was going to be so boring, I would have brought a book.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases['context'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ea874421-c88b-4d34-bae2-e6c2ca910f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If I had studied harder for the exam, I would have passed with flying colors.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases['response'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d18c0-b349-42a7-9619-427d86ed509f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b887067c-3f82-459f-98ff-14db6836fd1d",
   "metadata": {},
   "source": [
    "# Exp020: Eliciting target grammar skills\n",
    "This is an analysis for relationships between grammar skills that may be exploited for making learners produce desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacfa615-d1d4-41e7-8627-9ec06df0f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /scratch/tmp.58240468.dglandorf...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /scratch/tmp.58240468.dglandorf...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from '/cluster/home/dglandorf/grammarctg/experiments/../source/helpers.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ['CACHE_DIR'] = os.environ['FAST_CACHE_DIR'].replace(\"%SLURM_JOB_ID%\", os.getenv('SLURM_JOB_ID')) # speed up model loading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "#from scipy.stats import norm\n",
    "import scipy.stats as sps\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'../source')\n",
    "import data\n",
    "import evaluation\n",
    "import helpers\n",
    "import models\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc3fa3-2f61-40f0-b9b6-0634931cf5e6",
   "metadata": {},
   "source": [
    "Load dialogs with their turns that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e0ad03-031a-4471-8bad-b3b24d750aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs = data.get_dialog_data()\n",
    "turns = helpers.flatten_list_of_lists([d[0] for d in dialogs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0b12c-490f-4e06-9c8f-2c22251ccece",
   "metadata": {},
   "source": [
    "The attention defines which other turns are of interest in this paradigm. Attention is only paid to next turns of the interlocutur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3c30dd-c1fc-4379-9148-70f63df48076",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_turns = [len(d[0]) for d in dialogs]\n",
    "turn_attention = []\n",
    "n_resp = 2\n",
    "idx = 0\n",
    "for length in n_turns:\n",
    "    for i in range(0, length):\n",
    "        attention = []\n",
    "        for k in range(i, min(length-1, i+(2*n_resp-1)), 2): # only attend to next n response\n",
    "            attention += [idx+k+1]\n",
    "        turn_attention.append(attention)\n",
    "    idx += length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce794aea-ab9b-4a61-a84f-f73f5e9755d2",
   "metadata": {},
   "source": [
    "Let's classify the grammar in all turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae85efd3-e078-4cb1-9d93-20bcb37aa412",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = helpers.get_high_conf_classifiers()\n",
    "classifiers = {nr: models.load_classifier(nr, \"corpus_training\") for nr in skills}\n",
    "classified_file = '../data/turn_classification.pkl'\n",
    "primed_file ='../data/prime_stats.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd563a8-dc86-4def-981f-5b781b1f3233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b956f256444a6ea17a32eb15819f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentence tokenization:   0%|          | 0/710640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_turns = int(1e10)\n",
    "sentences = [(idx, sentence) for idx, turn in tqdm(enumerate(turns[:max_turns]), total=len(turns), desc=\"Sentence tokenization\") for sentence in data.sent_tokenize(turn)]\n",
    "indices, sents = [s[0] for s in sentences], [s[1] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b55532-4462-44c8-8ec4-722658992203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96990878197648fd93a5122fd6c214b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentence tokenization:   0%|          | 0/710640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c4bef296264b2ca214649591933cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grammar classification:   0%|          | 0/2371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_inputs = models.bert_tokenizer(sents, return_tensors='pt', max_length=64, padding='max_length', truncation=True)\n",
    "dataset = TensorDataset(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "clf_hits = {nr: [] for nr in skills}\n",
    "for input_ids, attention_mask in tqdm(dataloader, desc=\"Grammar classification\"):    \n",
    "    input_ids, attention_mask = input_ids.to(models.device), attention_mask.to(models.device)\n",
    "    with torch.no_grad():\n",
    "        encoded_inputs = models.bert_encoder(input_ids, attention_mask) # encoding is the same for all classifiers\n",
    "        x = torch.cat(encoded_inputs.hidden_states, dim=-1)\n",
    "        for nr, clf in classifiers.items():\n",
    "            max_values, _ = clf.forward_bert(x, attention_mask)\n",
    "            clf_hits[nr] += (max_values>0.5).cpu().tolist()\n",
    "\n",
    "with open(classified_file, 'wb') as f:\n",
    "    pickle.dump(clf_hits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ef70a3-4691-4225-ae2a-b3e58e1ad2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(classified_file, 'rb') as f:\n",
    "    clf_hits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a004a-a8ce-4fbf-8671-f6d3a9513c0c",
   "metadata": {},
   "source": [
    "Transform the classification into one big indicator matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db314707-07d7-4e6c-84c1-c963f9ef1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = np.zeros([sum(n_turns), len(skills)], dtype=bool)\n",
    "for idx, (nr, clf_hit) in enumerate(clf_hits.items()):\n",
    "    hit_indices = np.array(indices)[clf_hit]\n",
    "    hits[hit_indices,idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343dc8a8-c212-436e-8066-f3c19295f6f3",
   "metadata": {},
   "source": [
    "Calculate base rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a3bfb60-5650-4199-96d8-da930dfb889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0002941 , 0.01075509, 0.0044284 , 0.00390352, 0.00116937,\n",
       "       0.00012946, 0.00939013, 0.0122453 , 0.05582433, 0.00396825,\n",
       "       0.00505882, 0.00543876, 0.00704435, 0.00227541, 0.00202071,\n",
       "       0.00289035, 0.0016844 , 0.00027159, 0.00093296, 0.00075144,\n",
       "       0.00412023, 0.00024344, 0.04089835, 0.04483001, 0.01540724,\n",
       "       0.01069177, 0.027077  , 0.01082123, 0.02725571, 0.0012932 ,\n",
       "       0.00024204])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rate = hits.sum(axis=0)/min(max_turns, len(turns))\n",
    "base_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c078e4-4f29-4242-92d9-9b7458de4b22",
   "metadata": {},
   "source": [
    "Do a significance test for a changed proportion of target skills after prime skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "733490e8-5367-42cd-8be1-cfb4ff37535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = lambda z: (1 - norm.cdf(z)) # one-sided p-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c1a988-80f5-43f5-aac5-4c433b84cd50",
   "metadata": {},
   "source": [
    "Iterate through the primes and count targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8cdcca38-2db0-4759-abe0-ced631b8e650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "[]\n",
      "58\n",
      "[]\n",
      "59\n",
      "[(619, 0.05517151756404602)]\n",
      "69\n",
      "[]\n",
      "76\n",
      "[(619, 0.08036823275103122)]\n",
      "77\n",
      "[(619, 0.05790910283707562)]\n",
      "616\n",
      "[(618, 0.06882724174979478), (619, 0.0505452864752504), (621, 0.05040137953874835)]\n",
      "618\n",
      "[(616, 0.05976340463734406), (618, 0.09141164875521098), (619, 0.09251231866726402)]\n",
      "619\n",
      "[(619, 0.11644785777734987)]\n",
      "621\n",
      "[(616, 0.13242198222524254), (618, 0.13460310497978145), (619, 0.1952559181658533)]\n",
      "624\n",
      "[(619, 0.12394187955191875)]\n",
      "625\n",
      "[(619, 0.16237630283761123)]\n",
      "628\n",
      "[(619, 0.19008928435543956)]\n",
      "629\n",
      "[(619, 0.11913369409137246)]\n",
      "630\n",
      "[(619, 0.12279070214630791)]\n",
      "631\n",
      "[(619, 0.11236684982459001)]\n",
      "634\n",
      "[(619, 0.06887603078948645)]\n",
      "635\n",
      "[(619, 0.08710085033340675), (625, 0.05209106972507497), (1176, 0.06339578735268539)]\n",
      "636\n",
      "[(619, 0.1110765530599654)]\n",
      "1106\n",
      "[]\n",
      "1112\n",
      "[(619, 0.17973786463180114)]\n",
      "1116\n",
      "[(625, 0.12868330059657274), (631, 0.11016558893300175), (1116, 0.09207586423835384), (1175, 0.13131698455446528), (1179, 0.11341397519703539)]\n",
      "1175\n",
      "[]\n",
      "1176\n",
      "[]\n",
      "1177\n",
      "[]\n",
      "1178\n",
      "[]\n",
      "1179\n",
      "[(1179, 0.06261063191922273)]\n",
      "1187\n",
      "[]\n",
      "1192\n",
      "[]\n",
      "1197\n",
      "[]\n",
      "1198\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "all_stats = pd.DataFrame()\n",
    "for idx, nr in enumerate(skills):\n",
    "    print(nr)\n",
    "    turns_with_skill = set(np.where(hits[:,idx])[0])\n",
    "    antecedent_skills = np.vstack([hits[turn_attention[turn_nr],:].any(axis=0) for turn_nr in turns_with_skill])\n",
    "    other_skills = np.vstack([hits[turn_attention[turn_nr],:].any(axis=0) for turn_nr in range(len(turns)) if turn_nr not in turns_with_skill and turn_nr+2 not in turns_with_skill])\n",
    "    \n",
    "    x1 = antecedent_skills.sum(axis=0)\n",
    "    n1 = len(antecedent_skills)\n",
    "    p1 = x1 / n1\n",
    "    x2 = other_skills.sum(axis=0)\n",
    "    n2 = len(other_skills)\n",
    "    p2 = x2 / n2\n",
    "    \n",
    "    stats = pd.DataFrame({'prime': nr, \"target\": skills,\n",
    "                          \"x1\":x1, \"n1\":n1, \"p1\":p1,\n",
    "                          \"x2\":x2, \"n2\":n2, \"p2\":p2,\n",
    "                          \"p1-p2\": p1-p2})\n",
    "    all_stats = pd.concat([all_stats, stats])\n",
    "\n",
    "    #p_pool = (x1 + x2) / (n1 + n2)\n",
    "    #SE = np.sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
    "    #z = (p1 - p2) / SE\n",
    "    #p = p_val(z)\n",
    "    indices = np.where((p1-p2)>0.05)\n",
    "    print(list(zip(np.array(skills)[indices], (p1-p2)[indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f268ecd-80a5-4eb8-82b3-46fa806d848a",
   "metadata": {},
   "source": [
    "Test for significant differences with Fisher's exact test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8fa9afcf-efe0-41cf-aa00-b63b003ab438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value(row):\n",
    "    table=[[row['x1'], row['n1']-row['x1']],\n",
    "           [row['x2'], row['n2']-row['x2']]]\n",
    "    odds_ratio, p_value = sps.fisher_exact(table, \"greater\")\n",
    "    return pd.Series({'OR': odds_ratio, 'p': p_value})\n",
    "\n",
    "all_stats[['OR', 'p']] = all_stats.apply(get_p_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "beabc0ba-1a87-488d-b60a-2ea4c74a402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats.reset_index(drop=True).to_json(primed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf005f6-1db3-4408-a8af-57bb3be27386",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = pd.read_json(primed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd08ea52-40f6-4e81-83e7-4ec0cd82ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.202913631633715e-05\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05 / len(all_stats)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16906e3b-1a69-4870-b1dc-f28c1caa1416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prime</th>\n",
       "      <th>target</th>\n",
       "      <th>x1</th>\n",
       "      <th>n1</th>\n",
       "      <th>p1</th>\n",
       "      <th>x2</th>\n",
       "      <th>n2</th>\n",
       "      <th>p2</th>\n",
       "      <th>p1-p2</th>\n",
       "      <th>OR</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>209</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>324</td>\n",
       "      <td>710230</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.033037</td>\n",
       "      <td>75.928096</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>410</td>\n",
       "      <td>7643</td>\n",
       "      <td>0.053644</td>\n",
       "      <td>11956</td>\n",
       "      <td>695745</td>\n",
       "      <td>0.017184</td>\n",
       "      <td>0.036459</td>\n",
       "      <td>3.241915</td>\n",
       "      <td>8.541237e-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>103</td>\n",
       "      <td>3147</td>\n",
       "      <td>0.032730</td>\n",
       "      <td>4991</td>\n",
       "      <td>704448</td>\n",
       "      <td>0.007085</td>\n",
       "      <td>0.025645</td>\n",
       "      <td>4.742049</td>\n",
       "      <td>1.343990e-35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>53</td>\n",
       "      <td>2774</td>\n",
       "      <td>0.019106</td>\n",
       "      <td>4530</td>\n",
       "      <td>705171</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>3.012622</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>18</td>\n",
       "      <td>831</td>\n",
       "      <td>0.021661</td>\n",
       "      <td>1357</td>\n",
       "      <td>708986</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>11.545367</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>616</td>\n",
       "      <td>616</td>\n",
       "      <td>274</td>\n",
       "      <td>6673</td>\n",
       "      <td>0.041061</td>\n",
       "      <td>9075</td>\n",
       "      <td>697625</td>\n",
       "      <td>0.013008</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>3.248832</td>\n",
       "      <td>5.509896e-58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>618</td>\n",
       "      <td>618</td>\n",
       "      <td>926</td>\n",
       "      <td>8702</td>\n",
       "      <td>0.106412</td>\n",
       "      <td>10407</td>\n",
       "      <td>693769</td>\n",
       "      <td>0.015001</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>7.819518</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>619</td>\n",
       "      <td>619</td>\n",
       "      <td>7839</td>\n",
       "      <td>39671</td>\n",
       "      <td>0.197600</td>\n",
       "      <td>51632</td>\n",
       "      <td>636235</td>\n",
       "      <td>0.081152</td>\n",
       "      <td>0.116448</td>\n",
       "      <td>2.788296</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>621</td>\n",
       "      <td>621</td>\n",
       "      <td>33</td>\n",
       "      <td>2820</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>3686</td>\n",
       "      <td>705198</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>2.253496</td>\n",
       "      <td>2.861360e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>624</td>\n",
       "      <td>624</td>\n",
       "      <td>159</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.044228</td>\n",
       "      <td>5689</td>\n",
       "      <td>703555</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.036142</td>\n",
       "      <td>5.676493</td>\n",
       "      <td>1.960701e-63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>625</td>\n",
       "      <td>625</td>\n",
       "      <td>163</td>\n",
       "      <td>3865</td>\n",
       "      <td>0.042173</td>\n",
       "      <td>6850</td>\n",
       "      <td>703023</td>\n",
       "      <td>0.009744</td>\n",
       "      <td>0.032430</td>\n",
       "      <td>4.474843</td>\n",
       "      <td>8.491677e-52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>628</td>\n",
       "      <td>628</td>\n",
       "      <td>146</td>\n",
       "      <td>5006</td>\n",
       "      <td>0.029165</td>\n",
       "      <td>8106</td>\n",
       "      <td>700831</td>\n",
       "      <td>0.011566</td>\n",
       "      <td>0.017599</td>\n",
       "      <td>2.567266</td>\n",
       "      <td>1.670587e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>629</td>\n",
       "      <td>629</td>\n",
       "      <td>28</td>\n",
       "      <td>1617</td>\n",
       "      <td>0.017316</td>\n",
       "      <td>3008</td>\n",
       "      <td>707423</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>4.126529</td>\n",
       "      <td>1.200000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>630</td>\n",
       "      <td>630</td>\n",
       "      <td>20</td>\n",
       "      <td>1436</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>2611</td>\n",
       "      <td>707782</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.010239</td>\n",
       "      <td>3.814647</td>\n",
       "      <td>8.070000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>631</td>\n",
       "      <td>631</td>\n",
       "      <td>87</td>\n",
       "      <td>2054</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>3695</td>\n",
       "      <td>706573</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.037127</td>\n",
       "      <td>8.413572</td>\n",
       "      <td>3.135568e-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>634</td>\n",
       "      <td>634</td>\n",
       "      <td>15</td>\n",
       "      <td>1197</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>2111</td>\n",
       "      <td>708254</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.009551</td>\n",
       "      <td>4.245005</td>\n",
       "      <td>5.250800e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>636</td>\n",
       "      <td>636</td>\n",
       "      <td>9</td>\n",
       "      <td>663</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>1194</td>\n",
       "      <td>709323</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.011891</td>\n",
       "      <td>8.161553</td>\n",
       "      <td>2.692600e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>1112</td>\n",
       "      <td>1112</td>\n",
       "      <td>97</td>\n",
       "      <td>2928</td>\n",
       "      <td>0.033128</td>\n",
       "      <td>5214</td>\n",
       "      <td>704847</td>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.025731</td>\n",
       "      <td>4.597599</td>\n",
       "      <td>1.237501e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>1116</td>\n",
       "      <td>1116</td>\n",
       "      <td>16</td>\n",
       "      <td>173</td>\n",
       "      <td>0.092486</td>\n",
       "      <td>291</td>\n",
       "      <td>710302</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.092076</td>\n",
       "      <td>248.652264</td>\n",
       "      <td>1.405981e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>1175</td>\n",
       "      <td>1175</td>\n",
       "      <td>2964</td>\n",
       "      <td>29064</td>\n",
       "      <td>0.101982</td>\n",
       "      <td>44519</td>\n",
       "      <td>654339</td>\n",
       "      <td>0.068037</td>\n",
       "      <td>0.033945</td>\n",
       "      <td>1.555586</td>\n",
       "      <td>5.625353e-98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>1176</td>\n",
       "      <td>1176</td>\n",
       "      <td>3805</td>\n",
       "      <td>31858</td>\n",
       "      <td>0.119436</td>\n",
       "      <td>46750</td>\n",
       "      <td>649361</td>\n",
       "      <td>0.071994</td>\n",
       "      <td>0.047442</td>\n",
       "      <td>1.748360</td>\n",
       "      <td>7.171228e-189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>1177</td>\n",
       "      <td>1177</td>\n",
       "      <td>604</td>\n",
       "      <td>10949</td>\n",
       "      <td>0.055165</td>\n",
       "      <td>17419</td>\n",
       "      <td>689157</td>\n",
       "      <td>0.025276</td>\n",
       "      <td>0.029889</td>\n",
       "      <td>2.251558</td>\n",
       "      <td>1.040639e-65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>1178</td>\n",
       "      <td>1178</td>\n",
       "      <td>377</td>\n",
       "      <td>7598</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>11796</td>\n",
       "      <td>695615</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>0.032661</td>\n",
       "      <td>3.026568</td>\n",
       "      <td>6.498734e-71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>1179</td>\n",
       "      <td>1179</td>\n",
       "      <td>2071</td>\n",
       "      <td>19242</td>\n",
       "      <td>0.107629</td>\n",
       "      <td>30312</td>\n",
       "      <td>673323</td>\n",
       "      <td>0.045019</td>\n",
       "      <td>0.062611</td>\n",
       "      <td>2.558517</td>\n",
       "      <td>2.394976e-270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>1187</td>\n",
       "      <td>1187</td>\n",
       "      <td>240</td>\n",
       "      <td>7690</td>\n",
       "      <td>0.031209</td>\n",
       "      <td>11880</td>\n",
       "      <td>695544</td>\n",
       "      <td>0.017080</td>\n",
       "      <td>0.014129</td>\n",
       "      <td>1.853878</td>\n",
       "      <td>9.704874e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>1192</td>\n",
       "      <td>1192</td>\n",
       "      <td>1583</td>\n",
       "      <td>19369</td>\n",
       "      <td>0.081729</td>\n",
       "      <td>30310</td>\n",
       "      <td>673442</td>\n",
       "      <td>0.045008</td>\n",
       "      <td>0.036721</td>\n",
       "      <td>1.888499</td>\n",
       "      <td>5.731211e-106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>1197</td>\n",
       "      <td>1197</td>\n",
       "      <td>18</td>\n",
       "      <td>919</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>1652</td>\n",
       "      <td>708808</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.017256</td>\n",
       "      <td>8.551709</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prime  target    x1     n1        p1     x2      n2        p2     p1-p2  \\\n",
       "0       57      57     7    209  0.033493    324  710230  0.000456  0.033037   \n",
       "32      58      58   410   7643  0.053644  11956  695745  0.017184  0.036459   \n",
       "64      59      59   103   3147  0.032730   4991  704448  0.007085  0.025645   \n",
       "96      69      69    53   2774  0.019106   4530  705171  0.006424  0.012682   \n",
       "128     76      76    18    831  0.021661   1357  708986  0.001914  0.019747   \n",
       "192    616     616   274   6673  0.041061   9075  697625  0.013008  0.028053   \n",
       "224    618     618   926   8702  0.106412  10407  693769  0.015001  0.091412   \n",
       "256    619     619  7839  39671  0.197600  51632  636235  0.081152  0.116448   \n",
       "288    621     621    33   2820  0.011702   3686  705198  0.005227  0.006475   \n",
       "320    624     624   159   3595  0.044228   5689  703555  0.008086  0.036142   \n",
       "352    625     625   163   3865  0.042173   6850  703023  0.009744  0.032430   \n",
       "384    628     628   146   5006  0.029165   8106  700831  0.011566  0.017599   \n",
       "416    629     629    28   1617  0.017316   3008  707423  0.004252  0.013064   \n",
       "448    630     630    20   1436  0.013928   2611  707782  0.003689  0.010239   \n",
       "480    631     631    87   2054  0.042356   3695  706573  0.005229  0.037127   \n",
       "512    634     634    15   1197  0.012531   2111  708254  0.002981  0.009551   \n",
       "576    636     636     9    663  0.013575   1194  709323  0.001683  0.011891   \n",
       "640   1112    1112    97   2928  0.033128   5214  704847  0.007397  0.025731   \n",
       "672   1116    1116    16    173  0.092486    291  710302  0.000410  0.092076   \n",
       "704   1175    1175  2964  29064  0.101982  44519  654339  0.068037  0.033945   \n",
       "736   1176    1176  3805  31858  0.119436  46750  649361  0.071994  0.047442   \n",
       "768   1177    1177   604  10949  0.055165  17419  689157  0.025276  0.029889   \n",
       "800   1178    1178   377   7598  0.049618  11796  695615  0.016958  0.032661   \n",
       "832   1179    1179  2071  19242  0.107629  30312  673323  0.045019  0.062611   \n",
       "864   1187    1187   240   7690  0.031209  11880  695544  0.017080  0.014129   \n",
       "896   1192    1192  1583  19369  0.081729  30310  673442  0.045008  0.036721   \n",
       "928   1197    1197    18    919  0.019587   1652  708808  0.002331  0.017256   \n",
       "\n",
       "             OR              p  \n",
       "0     75.928096   0.000000e+00  \n",
       "32     3.241915   8.541237e-85  \n",
       "64     4.742049   1.343990e-35  \n",
       "96     3.012622   0.000000e+00  \n",
       "128   11.545367   0.000000e+00  \n",
       "192    3.248832   5.509896e-58  \n",
       "224    7.819518   0.000000e+00  \n",
       "256    2.788296   0.000000e+00  \n",
       "288    2.253496   2.861360e-05  \n",
       "320    5.676493   1.960701e-63  \n",
       "352    4.474843   8.491677e-52  \n",
       "384    2.567266   1.670587e-22  \n",
       "416    4.126529   1.200000e-09  \n",
       "448    3.814647   8.070000e-07  \n",
       "480    8.413572   3.135568e-48  \n",
       "512    4.245005   5.250800e-06  \n",
       "576    8.161553   2.692600e-06  \n",
       "640    4.597599   1.237501e-32  \n",
       "672  248.652264   1.405981e-32  \n",
       "704    1.555586   5.625353e-98  \n",
       "736    1.748360  7.171228e-189  \n",
       "768    2.251558   1.040639e-65  \n",
       "800    3.026568   6.498734e-71  \n",
       "832    2.558517  2.394976e-270  \n",
       "864    1.853878   9.704874e-18  \n",
       "896    1.888499  5.731211e-106  \n",
       "928    8.551709   0.000000e+00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stats[(all_stats['prime']==all_stats['target'])&(all_stats['p']<alpha)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fffd8ebd-0907-4d1f-b814-1afcc82d78a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56                                 FORM: 'MY BEST FRIEND'\n",
       "56                                 FORM: 'MY BEST FRIEND'\n",
       "56                                 FORM: 'MY BEST FRIEND'\n",
       "56                                 FORM: 'MY BEST FRIEND'\n",
       "56                                 FORM: 'MY BEST FRIEND'\n",
       "                              ...                        \n",
       "1197    FORM/USE: 'NOT ONLY ... (BUT) ALSO' WITH INVER...\n",
       "1197    FORM/USE: 'NOT ONLY ... (BUT) ALSO' WITH INVER...\n",
       "1197    FORM/USE: 'NOT ONLY ... (BUT) ALSO' WITH INVER...\n",
       "1197    FORM/USE: 'NOT ONLY ... (BUT) ALSO' WITH INVER...\n",
       "1197    FORM/USE: 'NOT ONLY ... (BUT) ALSO' WITH INVER...\n",
       "Name: guideword, Length: 961, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egp = helpers.egp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6235af81-a81e-4e47-a0c0-0666704f0177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prime_desc</th>\n",
       "      <th>target_desc</th>\n",
       "      <th>p2</th>\n",
       "      <th>p1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>would - QUESTIONS WITH 'LIKE'</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.093042</td>\n",
       "      <td>0.288298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>would - QUESTIONS</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.091772</td>\n",
       "      <td>0.281862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>conditional - 'IF' + PAST SIMPLE + 'WOULD', FU...</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.092803</td>\n",
       "      <td>0.272541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>would - AFTER 'IF' CLAUSES</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.092475</td>\n",
       "      <td>0.254851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>would - QUESTIONS WITH 'LIKE'</td>\n",
       "      <td>would - WISHES AND PREFERENCES WITH 'LIKE'</td>\n",
       "      <td>0.016106</td>\n",
       "      <td>0.150709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>would - QUESTIONS WITH 'LIKE'</td>\n",
       "      <td>would - AFFIRMATIVE WITH 'LIKE'</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.145035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>conditional - 'IF' + PAST PERFECT + 'WOULD HAV...</td>\n",
       "      <td>negation - MAIN VERB 'BE'</td>\n",
       "      <td>0.070995</td>\n",
       "      <td>0.202312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>conditional - 'IF' + PAST PERFECT + 'WOULD HAV...</td>\n",
       "      <td>would - AFTER 'IF' CLAUSES</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.138728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>would - WISHES AND PREFERENCES</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.093026</td>\n",
       "      <td>0.216968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>would - FUTURE IN THE PAST</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.093783</td>\n",
       "      <td>0.216574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>would - WITH ADVERBS</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.093606</td>\n",
       "      <td>0.212740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.081152</td>\n",
       "      <td>0.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>conditional - 'IF' + PAST PERFECT + 'WOULD HAV...</td>\n",
       "      <td>negation - AUXILIARY VERB 'DO', PAST</td>\n",
       "      <td>0.048436</td>\n",
       "      <td>0.161850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>would - IMAGINED SITUATIONS IN THE PAST</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.093573</td>\n",
       "      <td>0.205940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>would - HABITUAL PAST</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.094052</td>\n",
       "      <td>0.205128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>conditional - 'IF' + PAST PERFECT + 'WOULD HAV...</td>\n",
       "      <td>would - IMAGINED SITUATIONS IN THE PAST</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.115607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>would - WISHES AND PREFERENCES WITH 'LIKE'</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.092273</td>\n",
       "      <td>0.184785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>conditional - 'IF' + PAST PERFECT + 'WOULD HAV...</td>\n",
       "      <td>conditional - 'IF' + PAST PERFECT + 'WOULD HAV...</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.092486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>would - WISHES AND PREFERENCES WITH 'LIKE'</td>\n",
       "      <td>would - WISHES AND PREFERENCES WITH 'LIKE'</td>\n",
       "      <td>0.015001</td>\n",
       "      <td>0.106412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>superlatives - WITH NOUN AND 'TO-' INFINITIVE</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.094120</td>\n",
       "      <td>0.174489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>would - REPORTED SPEECH</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.094031</td>\n",
       "      <td>0.162907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>would - AFFIRMATIVE WITH 'LIKE'</td>\n",
       "      <td>would - WISHES AND PREFERENCES WITH 'LIKE'</td>\n",
       "      <td>0.015842</td>\n",
       "      <td>0.084670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>negation - AUXILIARY VERB 'DO', PAST</td>\n",
       "      <td>negation - AUXILIARY VERB 'DO', PAST</td>\n",
       "      <td>0.045019</td>\n",
       "      <td>0.107629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>would - WISHES AND PREFERENCES WITH 'LIKE'</td>\n",
       "      <td>would - AFFIRMATIVE WITH 'LIKE'</td>\n",
       "      <td>0.012174</td>\n",
       "      <td>0.071937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>superlatives - WITH 'IN' + NOUN</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.093859</td>\n",
       "      <td>0.149031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>would - WILLINGNESS IN THE PAST</td>\n",
       "      <td>would - AFTER 'IF' CLAUSES</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.062176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>would - AFFIRMATIVE WITH 'LIKE'</td>\n",
       "      <td>would - AFFIRMATIVE</td>\n",
       "      <td>0.093318</td>\n",
       "      <td>0.143863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>would - AFFIRMATIVE WITH 'LIKE'</td>\n",
       "      <td>would - QUESTIONS WITH 'LIKE'</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.054848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            prime_desc  \\\n",
       "287                      would - QUESTIONS WITH 'LIKE'   \n",
       "380                                  would - QUESTIONS   \n",
       "628  conditional - 'IF' + PAST SIMPLE + 'WOULD', FU...   \n",
       "349                         would - AFTER 'IF' CLAUSES   \n",
       "286                      would - QUESTIONS WITH 'LIKE'   \n",
       "285                      would - QUESTIONS WITH 'LIKE'   \n",
       "673  conditional - 'IF' + PAST PERFECT + 'WOULD HAV...   \n",
       "662  conditional - 'IF' + PAST PERFECT + 'WOULD HAV...   \n",
       "318                    would - WISHES AND PREFERENCES    \n",
       "442                         would - FUTURE IN THE PAST   \n",
       "411                               would - WITH ADVERBS   \n",
       "256                                would - AFFIRMATIVE   \n",
       "677  conditional - 'IF' + PAST PERFECT + 'WOULD HAV...   \n",
       "473            would - IMAGINED SITUATIONS IN THE PAST   \n",
       "566                              would - HABITUAL PAST   \n",
       "666  conditional - 'IF' + PAST PERFECT + 'WOULD HAV...   \n",
       "225         would - WISHES AND PREFERENCES WITH 'LIKE'   \n",
       "672  conditional - 'IF' + PAST PERFECT + 'WOULD HAV...   \n",
       "224         would - WISHES AND PREFERENCES WITH 'LIKE'   \n",
       "132      superlatives - WITH NOUN AND 'TO-' INFINITIVE   \n",
       "504                            would - REPORTED SPEECH   \n",
       "193                    would - AFFIRMATIVE WITH 'LIKE'   \n",
       "832               negation - AUXILIARY VERB 'DO', PAST   \n",
       "223         would - WISHES AND PREFERENCES WITH 'LIKE'   \n",
       "70                     superlatives - WITH 'IN' + NOUN   \n",
       "538                    would - WILLINGNESS IN THE PAST   \n",
       "194                    would - AFFIRMATIVE WITH 'LIKE'   \n",
       "195                    would - AFFIRMATIVE WITH 'LIKE'   \n",
       "\n",
       "                                           target_desc        p2        p1  \n",
       "287                                would - AFFIRMATIVE  0.093042  0.288298  \n",
       "380                                would - AFFIRMATIVE  0.091772  0.281862  \n",
       "628                                would - AFFIRMATIVE  0.092803  0.272541  \n",
       "349                                would - AFFIRMATIVE  0.092475  0.254851  \n",
       "286         would - WISHES AND PREFERENCES WITH 'LIKE'  0.016106  0.150709  \n",
       "285                    would - AFFIRMATIVE WITH 'LIKE'  0.012613  0.145035  \n",
       "673                          negation - MAIN VERB 'BE'  0.070995  0.202312  \n",
       "662                         would - AFTER 'IF' CLAUSES  0.010045  0.138728  \n",
       "318                                would - AFFIRMATIVE  0.093026  0.216968  \n",
       "442                                would - AFFIRMATIVE  0.093783  0.216574  \n",
       "411                                would - AFFIRMATIVE  0.093606  0.212740  \n",
       "256                                would - AFFIRMATIVE  0.081152  0.197600  \n",
       "677               negation - AUXILIARY VERB 'DO', PAST  0.048436  0.161850  \n",
       "473                                would - AFFIRMATIVE  0.093573  0.205940  \n",
       "566                                would - AFFIRMATIVE  0.094052  0.205128  \n",
       "666            would - IMAGINED SITUATIONS IN THE PAST  0.005441  0.115607  \n",
       "225                                would - AFFIRMATIVE  0.092273  0.184785  \n",
       "672  conditional - 'IF' + PAST PERFECT + 'WOULD HAV...  0.000410  0.092486  \n",
       "224         would - WISHES AND PREFERENCES WITH 'LIKE'  0.015001  0.106412  \n",
       "132                                would - AFFIRMATIVE  0.094120  0.174489  \n",
       "504                                would - AFFIRMATIVE  0.094031  0.162907  \n",
       "193         would - WISHES AND PREFERENCES WITH 'LIKE'  0.015842  0.084670  \n",
       "832               negation - AUXILIARY VERB 'DO', PAST  0.045019  0.107629  \n",
       "223                    would - AFFIRMATIVE WITH 'LIKE'  0.012174  0.071937  \n",
       "70                                 would - AFFIRMATIVE  0.093859  0.149031  \n",
       "538                         would - AFTER 'IF' CLAUSES  0.010085  0.062176  \n",
       "194                                would - AFFIRMATIVE  0.093318  0.143863  \n",
       "195                      would - QUESTIONS WITH 'LIKE'  0.004447  0.054848  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_desc(nrs):\n",
    "    egp_desc = egp.loc[nrs-1]\n",
    "    return list(egp_desc['SubCategory'] + \" - \" + egp_desc['guideword'].str.replace('^.*?: ', '', regex=True))\n",
    "all_stats['prime_desc'] = get_desc(all_stats['prime'])\n",
    "all_stats['target_desc'] = get_desc(all_stats['target'])\n",
    "all_stats[(all_stats['p']<alpha) & (all_stats['p1-p2']>0.05)].sort_values(\"p1-p2\", ascending=False)[['prime_desc','target_desc','p2','p1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc663174-c844-4c7a-87ac-03cec98ad94a",
   "metadata": {},
   "source": [
    "What is the maximum prime per skill?!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43f5b0-d9c6-4c71-ad2e-0b8349751fb9",
   "metadata": {},
   "source": [
    "## Establish causality by simulating an intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b79ae-1cba-41df-a8fd-fb9168fa194a",
   "metadata": {},
   "source": [
    "Now let's use one of them in a random dialog context and simulate the next answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137e4c9b-d084-48f8-92de-0a815295c33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79ff13174834353a3bed05fffe6d0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627e2b5243b24c3db56cac119e7b75bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bb8dfe84674dacaee24b7a4ad20cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685d79dfec494e3d9213ce7f4238be51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0469731dda4090a1aa3a65979b9ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c531544d8db4d22964c4d85a27d9887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8053c21b91422bbd337b1de5fdbd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca5ee9e55cb4af78fa9698c48f472e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a132fced4144beb4c3cb69e78ed88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed4dcc5cc8d46dd95d6132bb551d0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f10f20b83ea476a8cf34fa4291782a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792c70e7760142a4871e49d9f4188f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = models.load_generator(\"meta-llama/Meta-Llama-3-8B-Instruct\")#\"/cluster/home/dglandorf/models/llama-FT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f0cf9-b4dc-4a35-b271-502de1908da6",
   "metadata": {},
   "source": [
    "Sample random contexts and override the constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f0acf22-6f05-49db-98ec-3178eb884f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "nr = 621\n",
    "cases = pd.DataFrame([helpers.sample_dialog_snippet(dialogs) for _ in range(n)])\n",
    "cases.columns = ['context','response','source','id']\n",
    "cases['constraints']=[[nr]] * n\n",
    "cases = cases.apply(lambda x: helpers.get_generation_prompt(x, tokenizer.apply_chat_template, system_msg=True), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2f010-06f9-42e0-bb66-5b8f4582f475",
   "metadata": {},
   "source": [
    "Generate constrained answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b922820b-2bfa-434c-888e-a4b87637c965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate:   0%|                                                                                                                                      | 0/8 [00:00<?, ?it/s]2024-05-09 11:14:03.470815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generate: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [01:02<00:00,  7.83s/it]\n"
     ]
    }
   ],
   "source": [
    "cases['response'] = models.generate(model, tokenizer, list(cases['prompt']), verbose=False, do_sample=False, eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")], batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e0253-5bcf-4aff-847e-a8ea703dfb32",
   "metadata": {},
   "source": [
    "Append the answers to the context that fulfill the constraint and create prompts for an unconstrained answer to simulate the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34738efa-b5d3-4e6a-8f04-2be9d7586019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "print(len(cases))\n",
    "cases = cases[(models.probe_model(classifiers[nr], list(cases['response']))[0] > 0.5).numpy()]\n",
    "print(len(cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76d8e174-7df0-454e-82e6-dadaf8c71b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases['context'] = cases.apply(lambda x: x['context'] + [x['response'].replace(\"A: \", \"\")], axis=1)\n",
    "cases = cases.apply(lambda x: helpers.get_generation_prompt(x, tokenizer.apply_chat_template, system_msg=True, unconstrained=True), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a49779-7fdc-47ad-98ed-c55c543e6d7d",
   "metadata": {},
   "source": [
    "Generate next response again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2f4038d-5b13-4c39-a0e2-284a44def3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [01:19<00:00,  9.94s/it]\n"
     ]
    }
   ],
   "source": [
    "cases['response'] = models.generate(model, tokenizer, list(cases['prompt']), verbose=False, do_sample=False, eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3755ded1-d3ee-482c-b2df-760087132a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases['context'] = cases.apply(lambda x: x['context'] + [x['response'].replace(\"B: \", \"\")], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51405747-56d0-465b-a92b-dc105987bd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes that was until 1952 along with sculpture, music and painting',\n",
       " 'Yeah. DO you go to the library much?',\n",
       " 'I go sometimes. Did you hear about that guy in prison that proved his own innocence?',\n",
       " 'Yes he studies at the library and then also helped other people',\n",
       " 'Would you like to hear more about that story?',\n",
       " \"Here is a possible next turn of B:\\n\\nB: That sounds really interesting, I'd love to hear more about it.\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases['context'].sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91dbdb-19e1-4838-9870-f7ec8bbb270b",
   "metadata": {},
   "source": [
    "Test for target structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08cc1623-4e34-47d2-9436-5b3d3287021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n",
      "tensor(0.0635)\n",
      "618\n",
      "tensor(0.0794)\n",
      "619\n",
      "tensor(0.6667)\n"
     ]
    }
   ],
   "source": [
    "for nr in primed[nr]:\n",
    "    print(nr)\n",
    "    print((models.probe_model(classifiers[nr], list(cases['response']))[0]>0.5).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eaac85-1f95-4e2a-87cf-ca359e32848a",
   "metadata": {},
   "source": [
    "Now as a loop for all primes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10f4dfe8-a7fb-4dc2-9607-c1b88a4c7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e912bdb-54e6-4652-a89a-ad1c8e36290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(cases):\n",
    "    cases['response'] = models.generate(model, tokenizer, list(cases['prompt']), verbose=False, do_sample=False, eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "489dac04-5ec0-4cdc-af1d-3a38e85fe3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_succesful(constraint, n=8):\n",
    "    cases = pd.DataFrame([helpers.sample_dialog_snippet(dialogs) for _ in range(n)])\n",
    "    cases.columns = ['context','response','source','id']\n",
    "    cases['constraints'] = [[constraint]] * n\n",
    "    cases = cases.apply(lambda x: helpers.get_generation_prompt(x, tokenizer.apply_chat_template, system_msg=True), axis=1)\n",
    "    generate_responses(cases)\n",
    "    success = (models.probe_model(classifiers[constraint], list(cases['response']))[0] > 0.5).numpy()\n",
    "    print(success.mean())\n",
    "    return cases[success]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b12649a7-b596-40bb-b1e7-907703057cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = all_stats[(all_stats['p']<alpha) & (all_stats['p1-p2']>0.05)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfca1ed8-cf1a-4496-a7d0-1127c9443840",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant['ratio']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5086eae9-c000-4605-ba97-c8279d736092",
   "metadata": {},
   "outputs": [],
   "source": [
    "prime_nr=619\n",
    "nr=619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e0b2124-c60f-4a51-ba76-e600b04dc77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9634991f-07ff-42f1-bc44-87bb8c32d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant.loc[(relevant['prime']==prime_nr) & (relevant['target']==nr),'ratio']=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0de68d41-146d-4d51-8c0c-d8e925bafa54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(models.probe_model(classifiers[nr], [\"I'd do that.\"])[0]>0.5).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "252dae92-5478-4dca-ba9e-b0a8f175d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "['It has got around 89% of rotten tomatoes rating if you prefer watching movies having good ratings.', 'hmmmm, i know it might be completely different in genre but this brings me back to times of THE CHRONICLES OF NARNIA', 'Wow 89% really is a high number. what else do you have for me this is getting interesting.', 'Critics have raved it as the best-animated musical to come out of Disney. This movie has so many likable characters like; the rugged iceman, the reindeer, and a naive snowman.', \"That's amazing! I've heard it's the most popular animated movie in Disney's history, and I've always wanted to see it.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n",
      "tensor(0.1143)\n",
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[\"I feel like going for a drink, it's been a long day.\", 'Great idea! Peter, I could use the drink.', 'How about the new bar across road?', 'Sounds good. The food there is fantastic, too.', \"That's the best place to go, I think, to relax and unwind after a day like this.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n",
      "tensor(0.)\n",
      "616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "['Thank you, Rick! As you can see, ladies and gentleman, we are here in beautiful Pebble Beach where the top golfers in the world are trying to win the grand prize of one million dollars!', \"Whoa, that's a lot of cash! Let's go to the course and see how Tiger Woods is doing.\", \"All right, we are here at the eighth hole. It's a par four, and has some very difficult hazards which many golfers find difficult to avoid. Although, I did see Jack Nicklaus hit a hole in one on this very same hole!\", \"Tiger Woods is about to tee off, and let's see if he has the same luck as Jack. Tiger is asking his caddie for his driver and, he seems to be very nervous.\", \"I would like to get a closer look at Tiger's swing and see if he can overcome his nerves and hit a great shot.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:38<00:00,  9.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618\n",
      "tensor(0.0938)\n",
      "619\n",
      "tensor(0.1562)\n",
      "621\n",
      "tensor(0.0312)\n",
      "618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[\"It's crazuy\", 'Just imagine a world with the Thought Police stopping you from thinking as an independent person. It would be sad. ', 'Yes, it would, when was the book released?', 'That sweet little dystopian novel was published in 1949. ', 'I would like to read it, it sounds really interesting.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:45<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n",
      "tensor(0.0270)\n",
      "618\n",
      "tensor(0.0541)\n",
      "619\n",
      "tensor(0.2162)\n",
      "619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "['Yeah they just need to focus on their citizens and provide for them. North Korea is also a tough place to live too. Kim Jong-un needs to better lead the workers party.', 'NK is a mess and will collapse. people are starving. how long can this last?', \"That's a good question. We also don't actually know how old Kim Jong-un is!\", 'He likes his mysteries. Everything about him sounds fishy.', 'I would think that if Kim Jong-un would focus more on the economy and less on his military, things would start to improve in North Korea.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:36<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n",
      "tensor(0.0938)\n",
      "621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "['I wonder what they\\'d make of me if I start playing \"All I Want For Christmas\", constantly in March.  They\\'d probably just think I was a lunatic.', 'Or maybe that you were thinking ahead.', 'Maybe I should really think ahead and get my Holiday shopping done now, play that song, and put up the tree.', 'I know people who start shopping after labor day', 'Would you like to join me in my holiday preparations, or are you going to stick to your post-Labor Day shoppers?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:34<00:00,  8.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n",
      "tensor(0.0312)\n",
      "618\n",
      "tensor(0.0312)\n",
      "619\n",
      "tensor(0.6875)\n",
      "624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "['Do you know much about their durability?  I know Hondas and Toyotas have a great reputation, but not so sure about the Chevrolets.', 'I like how Chevrolet has been around for a long time. The company was started in 1911 by Louis Chevrolet and William C Durant.', 'That certainly is a long time.  What type of car are you looking into?', \"Possibly a truck. Chevrolet originally started to compete with Ford's Model T. It then became the best selling car by 1929.\", \"I would prefer a truck with a good fuel economy, so I'm looking at the Chevrolet Silverado.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:39<00:00,  7.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n",
      "tensor(0.1515)\n",
      "625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "['Greece, officially known as the Hellenic Republic, has such a rich history.', 'Yes, it really does.  The cradle of democracy.  I studied a lot of Greek philosophy in school.', 'Oh, same, I learned about the Greco-Persian Wars which lasted from 5th to 4th centuries BC. What about you?', 'The wars are fascinating. Thermopylae and the Spartans stand are very cool.  ', \"If I were you, I would definitely visit the Acropolis in Athens, it's an incredible piece of history.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate:  20%|████████████████████████                                                                                                | 1/5 [00:14<00:59, 14.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(cases[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m cases \u001b[38;5;241m=\u001b[39m cases\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: helpers\u001b[38;5;241m.\u001b[39mget_generation_prompt(x, tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template, unconstrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, system_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m cases[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcases\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|eot_id|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nr \u001b[38;5;129;01min\u001b[39;00m relevant[relevant[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mprime_nr][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(nr)\n",
      "File \u001b[0;32m~/grammarctg/experiments/../source/models.py:273\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, prompts, eos_token_id, max_new_tokens, batch_size, verbose, skip_special_tokens, do_sample)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(model_input)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 273\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m                               \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(token_ids[:,model_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:],\n\u001b[1;32m    282\u001b[0m                                   skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m    283\u001b[0m                                   device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[38;5;241m-\u001b[39mbatch_size:])\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/transformers/generation/utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1510\u001b[0m         input_ids,\n\u001b[1;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/transformers/generation/utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1196\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1016\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1006\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1007\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         cache_position,\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/accelerate/hooks.py:161\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 161\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/accelerate/hooks.py:356\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[1;32m    347\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    348\u001b[0m             module,\n\u001b[1;32m    349\u001b[0m             name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m             tied_params_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map,\n\u001b[1;32m    354\u001b[0m         )\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_keys\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/accelerate/utils/operations.py:189\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 189\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/accelerate/utils/operations.py:190\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    189\u001b[0m         {\n\u001b[0;32m--> 190\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    192\u001b[0m         }\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.local/lib64/python3.11/site-packages/accelerate/utils/operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    156\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_cases = pd.DataFrame()\n",
    "for prime_nr in relevant['prime'].unique():\n",
    "    print(prime_nr)\n",
    "    ratios[prime_nr] = {}\n",
    "    cases = pd.DataFrame()\n",
    "    attempts = 0\n",
    "    while len(cases) < 32 and attempts < 16:\n",
    "        cases = pd.concat([cases, sample_succesful(prime_nr)], ignore_index=True)\n",
    "        attempts += 1\n",
    "    if not len(cases): continue\n",
    "    cases['context'] = cases.apply(lambda x: x['context'] + [x['response'].replace(\"A: \", \"\")], axis=1)\n",
    "    print(cases['context'].sample(1).iloc[0])\n",
    "    cases = cases.apply(lambda x: helpers.get_generation_prompt(x, tokenizer.apply_chat_template, unconstrained=True, system_msg=True), axis=1)\n",
    "    cases['response'] = models.generate(model, tokenizer, list(cases['prompt']), verbose=False, do_sample=False, eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")], batch_size=8)\n",
    "    \n",
    "    for nr in relevant[relevant['prime']==prime_nr]['target']:\n",
    "        print(nr)\n",
    "        ratios[prime_nr][nr] = (models.probe_model(classifiers[nr], list(cases['response']))[0]>0.5).float().mean()\n",
    "        print(ratios[prime_nr][nr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f9b85-0d63-4996-b261-e95c39ea409e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

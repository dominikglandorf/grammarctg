{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8aed03-cb38-492a-8fab-60d1e35b727c",
   "metadata": {},
   "source": [
    "# Exp 013: Create constraints and prompts for grammar-controlled text generation tasks\n",
    "This notebooks experiments on how to setup the different tasks and prompts GPT3.5 as a baseline to test the evaluation suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86dccf97-0992-4cf4-abd1-4fb96ef2e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/scratch/dglandorf/cache...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/dglandorf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../source')\n",
    "import models\n",
    "import data\n",
    "import api\n",
    "import evaluation\n",
    "import importlib\n",
    "#importlib.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04b5921-b170-4436-8d8d-cf189826d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "levels = [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"] \n",
    "level_idx = {level: i for i, level in enumerate(levels)}\n",
    "\n",
    "def skills(rules, statement=True):\n",
    "     if statement: return \"- \" + rules['SubCategory'] + \" - \" + rules['guideword'] + \": \" + rules['Can-do statement']\n",
    "     return rules['SubCategory'] + \" \" + rules['guideword']\n",
    "    \n",
    "def get_prompt(rules, snippet, negative_rules=None, dialog=True, all=False, level=None, statement=True):\n",
    "    sep = os.linesep if statement else \"; \"\n",
    "    if level:\n",
    "        constraints = f\"grammar skills on CEFR level {level}.\"\n",
    "    else: \n",
    "        exclude = f\"\"\"Do NOT apply the following grammar skills:\n",
    "    {sep.join(skills(negative_rules, statement=statement))}\n",
    "\"\"\" if negative_rules else \"\"\n",
    "        constraints = f\"\"\"{\"all\" if all else \"at least one\"} of these grammar skills:\n",
    "{sep.join(skills(rules, statement=statement))}\n",
    "{exclude}\"\"\"\n",
    "    return f\"\"\"Continue the {\"dialog with one turn\" if dialog else \"text with two sentences\"}, proving knowledge of {constraints}\n",
    "Snippet:\n",
    "{snippet}\"\"\"\n",
    "    \n",
    "def classify_sents(classifiers, rules, sentences, verbose=True):\n",
    "    results = []\n",
    "    for idx, rule in rules.iterrows():\n",
    "        scores = models.probe_model(classifiers[rule['#']], sentences)\n",
    "        hits = [sentences[i] for i, score in enumerate(scores[0]) if score > 0.5]\n",
    "        if verbose & len(hits): print(f'\\n{rule[\"Level\"]}-{rule[\"Can-do statement\"]} ({rule[\"#\"]})\\n{os.linesep.join([\"- \"+h for h in hits])}')\n",
    "        results.append(len(hits)/len(sentences))\n",
    "    return results\n",
    "\n",
    "def sample_dialog_snippet(dialogs, n=4):\n",
    "    dialog = []\n",
    "    while len(dialog) < n+1:\n",
    "        dialog = random.sample(dialogs, 1)[0]\n",
    "    index = random.randint(0, len(dialog) - n)\n",
    "    utterances = dialog[index:index+n]\n",
    "    return os.linesep.join([(\"A\" if (i%2==0) else \"B\") + \": \" + utt for i, utt in enumerate(utterances[:-1] + [\"\"])]), utterances[-1]\n",
    "\n",
    "def prompt_score(rules, snippet, classifiers, dialog, all, n_responses=1, level=None, statement=True, verbose=False):\n",
    "    prompt = get_prompt(rules, snippet, dialog=dialog, all=all, level=level, statement=statement)\n",
    "    if verbose: print(f\"PROMPT: {prompt}\")\n",
    "    responses = [api.get_openai_chat_completion([{ \"role\": \"user\", \"content\": prompt}])[0] for _ in range(n_responses)] #, model=\"gpt-4-0125-preview\"\n",
    "    if verbose: print(\"RESPONSES from GPT3.5\")\n",
    "    if verbose: print(responses)\n",
    "    if dialog: print(evaluation.evaluate_responses([snippet], [responses], [list(rules['#'])], [[]]))\n",
    "    return [classify_sents(classifiers, rules, data.sent_tokenize(response), verbose) for response in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720d72c-001b-4258-acad-9f95a46c2241",
   "metadata": {},
   "source": [
    "Load EGP rules, topical prompts and dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ecdf038-3b78-4ad5-87cc-7a84365738f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "egp = data.get_egp()\n",
    "cefr = data.CEFRTexts()\n",
    "stories = list(cefr.get_beginnings(100))\n",
    "ds = data.DialogSum()\n",
    "dd = data.DailyDialog()\n",
    "wow = data.WoW()\n",
    "dialogs = ds.get_dialogues() + dd.get_dialogues() + wow.get_dialogues()\n",
    "classifiers = {nr: models.load_classifier(nr, 'corpus_training') for nr in [int(name.replace(\".pth\",\"\")) for name in os.listdir(f\"../models/corpus_training\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9705fd-2edd-47c3-b833-88edfac04190",
   "metadata": {},
   "source": [
    "General parameters for task datasets\n",
    "1. text vs dialog -> dialog\n",
    "2. context length -> 4 turns (trade off between context length and informativeness)\n",
    "\n",
    "Dev datasets: Wizard of Wikipedia, Daily Dialog, Dialog Sum\n",
    "\n",
    "Test datasets: CMU Document grounded conversations\n",
    "\n",
    "# Questions\n",
    "\n",
    "- Should the dialog contexts be as diverse as possible, i.e. different for each datapoint, or should the same set of dialog contexts be included in every combination of parameters?\n",
    "- Is it reasonable to work with a fixed number of turns both in training and test data? I'd say there is no reason why it should not generalize.\n",
    "- Is it okay to include no or only an unconditioned ground truth answer in train and test set since all metrics are reference-free? Using only sets of constraints that are fulfilled in the given datasets will likely make the evaluation much smaller and less challenging.\n",
    "- In the evaluation, should I normalize the score by the number of sentences or should it give a full score if a constraint is met at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95833ffb-6fcf-41c5-aaf0-10042fc81a4c",
   "metadata": {},
   "source": [
    "## Task 1: Single constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed6ed12-babd-4267-a841-da3363c58c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_constraints(level=None, n_per_subcat=1, subcategories = [\"would\", \"negation\", \"superlatives\"]):\n",
    "    if level is None: level = random.sample(levels, 1)[0]\n",
    "    return egp[(egp['SubCategory'].isin(subcategories)) & (egp['Level']==level) & egp['#'].isin(classifiers.keys())].groupby(\"SubCategory\").sample(n_per_subcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5231eb05-cefc-4ebd-96e1-b8bd76e7e0e2",
   "metadata": {},
   "source": [
    "For Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50759e8-9cbc-404e-b730-6748bbb46ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Continue the text with two sentences, proving knowledge of all of these grammar skills:\n",
      "- negation - FORM/USE: 'NOT ONLY ... (BUT) ALSO' WITH INVERSION: Can use auxiliary 'do' + inverted subject after 'not only', to give focus.\n",
      "- superlatives - FORM/USE: WITH NOUN AND POSTMODFIER: Can use a postmodifier to make the superlative stronger in the structure superlative + postmodifier + noun. \n",
      "- would - FORM: WITH ADVERBS: Can use an increasing range of adverbs with 'would', including 'strongly', 'easily', 'especially', 'actually', 'absolutely', 'gladly'  ► adverbs\n",
      "\n",
      "Snippet:\n",
      "Excuse me.\n",
      "RESPONSES from GPT3.5\n",
      "['Not only did she break the record for the fastest runner, but she also set a new world record, making her the fastest woman alive. She would absolutely dominate the competition, especially with her strong determination and training regimen.']\n",
      "\n",
      "C1-Can use auxiliary 'do' + inverted subject after 'not only', to give focus. (1198)\n",
      "- Not only did she break the record for the fastest runner, but she also set a new world record, making her the fastest woman alive.\n",
      "\n",
      "C1-Can use an increasing range of adverbs with 'would', including 'strongly', 'easily', 'especially', 'actually', 'absolutely', 'gladly'  ► adverbs (637)\n",
      "- She would absolutely dominate the competition, especially with her strong determination and training regimen.\n",
      "Constraint scores: [[0.5, 0.0, 0.5]]\n"
     ]
    }
   ],
   "source": [
    "snippet = random.sample(stories, 1)[0]\n",
    "rules = sample_single_constraints()\n",
    "scores = prompt_score(rules, snippet, classifiers, dialog=False, all=True, verbose=True)\n",
    "print(f\"Constraint scores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571a8ef-bbd2-467a-aefd-a66c11d9ac80",
   "metadata": {},
   "source": [
    "For Dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af418ca8-1e3b-44f8-a77c-d2ccbc37ddbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Continue the dialog with one turn, proving knowledge of all of these grammar skills:\n",
      "- negation - FORM: 'NOT ALL', 'NOT EVERY': Can use 'not with indefinite pronouns 'everyone' and 'everything' and determiners 'every', 'all'.\n",
      "- superlatives - FORM/USE: WITH NOUN AND POSTMODFIER: Can use a postmodifier to make the superlative stronger in the structure superlative + postmodifier + noun. \n",
      "- would - FORM: WITH ADVERBS: Can use an increasing range of adverbs with 'would', including 'strongly', 'easily', 'especially', 'actually', 'absolutely', 'gladly'  ► adverbs\n",
      "\n",
      "Snippet:\n",
      "A: Either way is good for me.\n",
      "B: There are a number of open houses this weekend in your area. Would that okay with you?\n",
      "A: I can take a little time off of work, or I can go on a weekend, also.\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['Not all of the open houses this weekend will be suitable for us, so we should carefully choose the ones that interest us the most.', \"Not every open house will have what I'm looking for, so I'll have to choose carefully.\", 'Not all of the open houses are within walking distance from my house, so I would probably need to drive, especially if the weather is bad.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a654bf5a0546f090dae6fb882ca5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.8904109589041096], 'positive_constraints': [[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[3.0, 4.0, 4.0]], 'Relevance': [[2.0, 4.0, 4.0]], 'Content Richness': [[3.0, 4.0, 3.0]], 'Grammatical Correctness': [[4.0, 5.0, 4.0]]}\n",
      "\n",
      "C1-Can use 'not with indefinite pronouns 'everyone' and 'everything' and determiners 'every', 'all'. (1197)\n",
      "- Not all of the open houses this weekend will be suitable for us, so we should carefully choose the ones that interest us the most.\n",
      "\n",
      "C1-Can use 'not with indefinite pronouns 'everyone' and 'everything' and determiners 'every', 'all'. (1197)\n",
      "- Not every open house will have what I'm looking for, so I'll have to choose carefully.\n",
      "\n",
      "C1-Can use 'not with indefinite pronouns 'everyone' and 'everything' and determiners 'every', 'all'. (1197)\n",
      "- Not all of the open houses are within walking distance from my house, so I would probably need to drive, especially if the weather is bad.\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of all of these grammar skills:\n",
      "- negation - FORM: 'DO', IMPERATIVES: Can form negative imperatives of main verbs with 'don't' + main verb. ► Clauses: imperatives\n",
      "- superlatives - FORM/USE: COMPLEX NOUN PHRASES: Can form a limited range of complex noun phrases with a superlative adjective  + prepositional phrase, to talk about something unique.► noun phrases ►  clauses: comparison\n",
      "- would - USE: SUGGESTIONS WITH 'IT WOULD BE': Can use 'it would be' to make suggestions.\n",
      "\n",
      "Snippet:\n",
      "A: Excuse me. May I take a picture of you and your little boy?\n",
      "B: What's it for?\n",
      "A: It's for a book.\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "[\"I'm sorry, but I'd rather not have our pictures taken.\", \"I'm sorry, but I'd rather not have our picture taken.\", \"I'm sorry, but I don't think that would be a good idea.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526d2b563d164ff79fb41a9fc10fb528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.6097560975609756], 'positive_constraints': [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 1.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 5.0, 2.0]], 'Relevance': [[4.0, 4.0, 4.0]], 'Content Richness': [[2.0, 2.0, 2.0]], 'Grammatical Correctness': [[5.0, 5.0, 5.0]]}\n",
      "\n",
      "A2-Can use 'it would be' to make suggestions. (623)\n",
      "- I'm sorry, but I don't think that would be a good idea.\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of all of these grammar skills:\n",
      "- negation - FORM: AUXILIARY VERBS 'BE', 'HAVE', PAST: Can form negative statements of main verbs in the past continuous and past perfect with auxiliary verbs 'be' and 'have' + 'not/n't'. ► past continuous ► past perfect\n",
      "- superlatives - FORM/USE: 'THE BEST' WITH NOUN AND PRESENT PERFECT: Can use 'the best' before a noun + present perfect to talk about a unique experience.\n",
      "- would - USE: WILLINGNESS IN THE PAST: Can use the negative forms of 'would' to talk about willingness in the past.\n",
      "\n",
      "Snippet:\n",
      "A: Do you know what the origin of the logo was or if it's just because of the name where the name comes from?\n",
      "B: I think its just the M in McDonald's. Oak Brook, Illinois was the original headquarters but they just moved it to Chicago this year. \n",
      "A: Hmm Do you know why they moved to Chicago? \n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['I believe they relocated to Chicago because it is one of the best locations for attracting top talent in the industry.', \"I'm not sure of the exact reason, but I heard that Chicago offered better incentives for the company to relocate.\", \"I'm not sure of the exact reason, but I believe Chicago was considered the best location for their global headquarters.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a645a4001d4016828a567c75349766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.8], 'positive_constraints': [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[3.0, 4.0, 4.0]], 'Relevance': [[3.0, 4.0, 4.0]], 'Content Richness': [[4.0, 4.0, 4.0]], 'Grammatical Correctness': [[5.0, 5.0, 5.0]]}\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of all of these grammar skills:\n",
      "- negation - FORM/USE: 'NOT', EMPHASIS: Can use uncontracted 'not' for emphasis or in formal contexts.\n",
      "- superlatives - FORM: ELLIPSIS, WITH 'THE': Can use '(one of) the' with an increasing range of superlative adjectives without a following noun, when the noun is understood.\n",
      "- would - USE: HABITUAL PAST: Can use 'would' to talk about habitual actions and events in the past.\n",
      "\n",
      "Snippet:\n",
      "A: Do you have a favorite side dish? I love mashed potatoes, it goes with so much!\n",
      "B: I love them too. I also love yellow rice\n",
      "A: Yellow rice is good, and I really like Spanish rice using chicken broth and a little tomato sauce.\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['That sounds delicious! Spanish rice is one of my favorite side dishes, I would always order it at my favorite Mexican restaurant.', 'That sounds delicious. Spanish rice is one of my favorite dishes to make. It always turned out perfectly.', \"That sounds delicious! Spanish rice is one of the best side dishes I've ever had.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e49318be0b648a3acff805ac202e036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.7049180327868853], 'positive_constraints': [[[0.0, 0.0, 0.5], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[5.0, 4.0, 4.0]], 'Content Richness': [[4.0, 4.0, 4.0]], 'Grammatical Correctness': [[5.0, 4.0, 5.0]]}\n",
      "\n",
      "B2-Can use 'would' to talk about habitual actions and events in the past. (636)\n",
      "- Spanish rice is one of my favorite side dishes, I would always order it at my favorite Mexican restaurant.\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of all of these grammar skills:\n",
      "- negation - FORM: MENTAL PROCESS VERBS + CLAUSE: Can use the negative forms of mental process verbs ('I don't think', 'I don't believe') followed by a complement clause, where the negative form is in the mental process verb rather than the complement clause.\n",
      "- superlatives - FORM/USE: 'THE BEST' WITH NOUN AND PRESENT PERFECT: Can use 'the best' before a noun + present perfect to talk about a unique experience.\n",
      "- would - FORM: PAST AFFIRMATIVE: Can use 'would have' + '-ed'.\n",
      "\n",
      "Snippet:\n",
      "A: What is the rent each month?\n",
      "B: It's only $ 725 a month.\n",
      "A: That's amazing. The rent is very cheap.\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "[\"I don't think I've ever seen a better deal in this city.\", \"I don't think I've ever seen a better deal than this!\", \"I don't think I could find a better deal anywhere else. It's the best I've ever seen.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e18ecd4ad14576b8242661f652d8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.6041666666666666], 'positive_constraints': [[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.5, 0.5, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[4.0, 4.0, 5.0]], 'Content Richness': [[4.0, 4.0, 4.0]], 'Grammatical Correctness': [[5.0, 5.0, 5.0]]}\n",
      "\n",
      "B1-Can use the negative forms of mental process verbs ('I don't think', 'I don't believe') followed by a complement clause, where the negative form is in the mental process verb rather than the complement clause. (1186)\n",
      "- I don't think I've ever seen a better deal in this city.\n",
      "\n",
      "B1-Can use the negative forms of mental process verbs ('I don't think', 'I don't believe') followed by a complement clause, where the negative form is in the mental process verb rather than the complement clause. (1186)\n",
      "- I don't think I've ever seen a better deal than this!\n",
      "\n",
      "B1-Can use the negative forms of mental process verbs ('I don't think', 'I don't believe') followed by a complement clause, where the negative form is in the mental process verb rather than the complement clause. (1186)\n",
      "- I don't think I could find a better deal anywhere else.\n",
      "\n",
      "B1-Can use 'the best' before a noun + present perfect to talk about a unique experience. (70)\n",
      "- It's the best I've ever seen.\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rules = sample_single_constraints()\n",
    "    snippet, _ = sample_dialog_snippet(dialogs)\n",
    "    prompt_score(rules, snippet, classifiers, dialog=True, all=True, verbose=True, n_responses=3)\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605181c-6251-4b01-88ec-893a3d77a6c0",
   "metadata": {},
   "source": [
    "Single constraints parameters\n",
    "1. number of single EGP skill constraints -> 1-6 (unreasonable to have more than six constraints in one answer)\n",
    "2. number of subcategories to choose constraints from -> 1-3 (cost of developing more classifiers)\n",
    "3. CEFR levels -> A1-C2\n",
    "\n",
    "dataset format\n",
    "\n",
    "CTX_SOURCE_DS | CTX_SOURCE_ID | CONTEXT | RESPONSE | EGP_NRS\n",
    "Daily Dialog | 023 | A: Hey B: Hey A: How are you? | B: I'm good | 616,636,1199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d05eca6-4587-436f-a5a3-166886e147c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: A: I do too! Bruce Almighty was hilarious\n",
      "B: Yes it was, he doesn't disappointed, I love his movies, including Dick and Jane and Truman Show!\n",
      "A: also has quite a few other big name actors and actresses like Freeman and Aniston!\n",
      "B: \n",
      "True response: Fun with Dick and Jane is one of his best in my opinion.\n",
      "Context: A: It is produced by Walt Disney Animation Studios, it is their 53rd featured film. Did you know it is inspired from a Hans Christian Anderson fairy tail?\n",
      "B: Oh, right... Which fairy tale is that?\n",
      "A: The fairy tale is called \"The Snow Queen\" - a story about a brave princess who sets out with a group of friends to finder her estranged sister.\n",
      "B: \n",
      "True response: That does sound like a good one. Do you know which actors provided voices for the characters?\n"
     ]
    }
   ],
   "source": [
    "subcats = [\"would\", \"negation\", \"superlatives\"]\n",
    "num_constraints = list(range(1,1+6))\n",
    "num_subcats = list(range(1,1+3))\n",
    "num_dialogs = 2\n",
    "random.seed(os.getenv(\"RANDOM_SEED\"))\n",
    "#dd = data.DialogSum()\n",
    "#dd = data.DailyDialog()\n",
    "#dd = data.WoW()\n",
    "dd = data.CMUDoG()\n",
    "dialogs = dd.get_dialogues()\n",
    "dialog_contexts = [sample_dialog_snippet(dialogs, n=4) for _ in range(num_dialogs)]\n",
    "\n",
    "for context, response in dialog_contexts:\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"True response: {response}\")\n",
    "    for num_constraint in num_constraints:\n",
    "        for num_subcat in num_subcats:\n",
    "            for level in levels:\n",
    "                pass\n",
    "                # add to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876556ad-af19-4d72-bcb9-50d58e944660",
   "metadata": {},
   "source": [
    "## Task 2: Combine subcategories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b77d9e-1ae4-4877-98c0-46fd26491794",
   "metadata": {},
   "source": [
    "Choose subcategory and level skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc8b7c4c-34ec-4548-8449-710f138a2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subcategory_rules(subcategories = [\"would\", \"negation\", \"superlatives\"], levels = [\"B1\", \"B2\", \"B1\"]):\n",
    "    filter_clf = (egp['#'].isin(classifiers.keys()))\n",
    "    return pd.concat([egp[(egp['SubCategory'] == subcat) & (egp['Level']==level) & filter_clf] for subcat, level in zip(subcategories, levels)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6268d6-c11e-4d6c-8a5c-37274318e001",
   "metadata": {},
   "source": [
    "Assemble prompt for a random story beginning, test it with GPT3.5 and use classifiers to score the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "275645fe-8cd0-48f7-a877-6089ac4b4c20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Continue the text with two sentences, proving knowledge of at least one of these grammar skills:\n",
      "would FORM/USE: AFTER 'IF' CLAUSES; would FORM: PAST AFFIRMATIVE; would FORM: PAST NEGATIVE; would FORM: QUESTIONS; would FORM: WITH ADVERBS; would USE: FUTURE IN THE PAST; would USE: IMAGINED SITUATIONS IN THE PAST; would USE: INDIRECTNESS; would USE: POLITE REQUESTS; would USE: REPORTED SPEECH; would USE: WILLINGNESS IN THE PAST; negation FORM/USE: 'NOT', EMPHASIS; negation FORM/USE: 'NEVER', INVERTED FRONT POSITION, FOCUS; negation FORM/USE: 'NEITHER ... NOR'; superlatives FORM/USE: 'THE BEST' WITH NOUN AND PRESENT PERFECT\n",
      "\n",
      "Snippet:\n",
      "Hume proposes that feeling, not thought, informs us that an object is beautiful or ugly, or that an action exhibits virtue or vice: “The very feeling constitutes our praise or admiration” (T, 471).\n",
      "RESPONSES from GPT3.5\n",
      "['If Hume were alive today, he would likely argue that beauty and morality remain subjective concepts influenced by individual perceptions shaped by feeling rather than rational thought. Furthermore, he would emphasize that our admiration for beauty and virtue is based on an inherent emotional response rather than a logical conclusion.']\n",
      "\n",
      "B1-Can use 'would' in the main clause of a conditional sentence to talk about an imagined situation, often in the context of advice or opinion-giving. (625)\n",
      "- If Hume were alive today, he would likely argue that beauty and morality remain subjective concepts influenced by individual perceptions shaped by feeling rather than rational thought.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = sample_subcategory_rules()\n",
    "snippet = random.sample(stories, 1)[0]\n",
    "prompt_score(rules, snippet, classifiers, dialog=False, all=False, verbose=True, statement=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcafb80-a1b7-497d-a5eb-ce2a8d46fea6",
   "metadata": {},
   "source": [
    "Do this with dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9211922e-4a41-484f-af63-2935d8ae034f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Continue the dialog with one turn, proving knowledge of at least one of these grammar skills:\n",
      "would FORM/USE: AFTER 'IF' CLAUSES; would FORM: PAST AFFIRMATIVE; would FORM: PAST NEGATIVE; would FORM: QUESTIONS; would FORM: WITH ADVERBS; would USE: FUTURE IN THE PAST; would USE: IMAGINED SITUATIONS IN THE PAST; would USE: INDIRECTNESS; would USE: POLITE REQUESTS; would USE: REPORTED SPEECH; would USE: WILLINGNESS IN THE PAST; negation FORM/USE: 'NOT', EMPHASIS; negation FORM/USE: 'NEVER', INVERTED FRONT POSITION, FOCUS; negation FORM/USE: 'NEITHER ... NOR'; superlatives FORM/USE: 'THE BEST' WITH NOUN AND PRESENT PERFECT\n",
      "\n",
      "Snippet:\n",
      "A: It's fun because they incorporate a lot of classic toys including a pull-string cowboy, Mr Potato Head, Green Army Men, and\n",
      " toy dinosaurs\n",
      "B: It sounds good.\n",
      "A: In one scene the green army men, led by the character Sarge spies on a party, and reports the\n",
      "findings to the other toys via baby monitors. It's light-hearted in its approach\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['That would have been a fun scene to watch, imagining toys spying on a party through baby monitors.', 'It would be interesting to see how the toys would react in a situation like that.', 'That would have been a clever way to include all the iconic toys in the movie, creating a nostalgic and entertaining experience for both children and adults.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a28a0c7da14c16a262fcb98b2dead8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.9365079365079365], 'positive_constraints': [[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[4.0, 4.0, 4.0]], 'Content Richness': [[4.0, 4.0, 4.0]], 'Grammatical Correctness': [[5.0, 4.0, 5.0]]}\n",
      "\n",
      "B1-Can use 'would have' + '-ed'. (626)\n",
      "- That would have been a fun scene to watch, imagining toys spying on a party through baby monitors.\n",
      "\n",
      "B1-Can use 'would' to talk about the future in the past. (630)\n",
      "- It would be interesting to see how the toys would react in a situation like that.\n",
      "\n",
      "B1-Can use 'would have' + '-ed'. (626)\n",
      "- That would have been a clever way to include all the iconic toys in the movie, creating a nostalgic and entertaining experience for both children and adults.\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of at least one of these grammar skills:\n",
      "would FORM/USE: AFTER 'IF' CLAUSES; would FORM: PAST AFFIRMATIVE; would FORM: PAST NEGATIVE; would FORM: QUESTIONS; would FORM: WITH ADVERBS; would USE: FUTURE IN THE PAST; would USE: IMAGINED SITUATIONS IN THE PAST; would USE: INDIRECTNESS; would USE: POLITE REQUESTS; would USE: REPORTED SPEECH; would USE: WILLINGNESS IN THE PAST; negation FORM/USE: 'NOT', EMPHASIS; negation FORM/USE: 'NEVER', INVERTED FRONT POSITION, FOCUS; negation FORM/USE: 'NEITHER ... NOR'; superlatives FORM/USE: 'THE BEST' WITH NOUN AND PRESENT PERFECT\n",
      "\n",
      "Snippet:\n",
      "A: did you read the document \n",
      "B: I did, abou tthe Blnd Side\n",
      "A: oh cool. what's it about?\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "[\"It's about a high school football player who overcomes obstacles with the help of a caring family.\", \"It's about the true story of Michael Oher, a football player who overcame numerous obstacles to achieve success.\", \"It's about a football player who overcomes obstacles and achieves success.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7237a90feb940a9b4235e3dc4c0fb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.74], 'positive_constraints': [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[4.0, 5.0, 5.0]], 'Content Richness': [[4.0, 4.0, 4.0]], 'Grammatical Correctness': [[5.0, 5.0, 5.0]]}\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of at least one of these grammar skills:\n",
      "would FORM/USE: AFTER 'IF' CLAUSES; would FORM: PAST AFFIRMATIVE; would FORM: PAST NEGATIVE; would FORM: QUESTIONS; would FORM: WITH ADVERBS; would USE: FUTURE IN THE PAST; would USE: IMAGINED SITUATIONS IN THE PAST; would USE: INDIRECTNESS; would USE: POLITE REQUESTS; would USE: REPORTED SPEECH; would USE: WILLINGNESS IN THE PAST; negation FORM/USE: 'NOT', EMPHASIS; negation FORM/USE: 'NEVER', INVERTED FRONT POSITION, FOCUS; negation FORM/USE: 'NEITHER ... NOR'; superlatives FORM/USE: 'THE BEST' WITH NOUN AND PRESENT PERFECT\n",
      "\n",
      "Snippet:\n",
      "A: yea i wouldnt mind watching it if i had the time\n",
      "B: The cast is actually pretty famous overall\n",
      "A: I think Margot Robbie does well as DiCaprios romantic partner in this movie\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['She is definitely one of the best actresses in Hollywood right now.', 'Yes, she would definitely be a great choice for that role.', \"She would definitely bring a level of charisma to any role she's in.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb2a28a190f453f97eb031f5a911e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.9736842105263158], 'positive_constraints': [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[4.0, 4.0, 4.0]], 'Content Richness': [[4.0, 2.0, 4.0]], 'Grammatical Correctness': [[4.0, 5.0, 4.0]]}\n",
      "\n",
      "B1-Can use an limited range of adverbs with 'would', including 'really', 'probably', 'certainly', 'definitely'.► adverbs (629)\n",
      "- Yes, she would definitely be a great choice for that role.\n",
      "\n",
      "B1-Can use an limited range of adverbs with 'would', including 'really', 'probably', 'certainly', 'definitely'.► adverbs (629)\n",
      "- She would definitely bring a level of charisma to any role she's in.\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of at least one of these grammar skills:\n",
      "would FORM/USE: AFTER 'IF' CLAUSES; would FORM: PAST AFFIRMATIVE; would FORM: PAST NEGATIVE; would FORM: QUESTIONS; would FORM: WITH ADVERBS; would USE: FUTURE IN THE PAST; would USE: IMAGINED SITUATIONS IN THE PAST; would USE: INDIRECTNESS; would USE: POLITE REQUESTS; would USE: REPORTED SPEECH; would USE: WILLINGNESS IN THE PAST; negation FORM/USE: 'NOT', EMPHASIS; negation FORM/USE: 'NEVER', INVERTED FRONT POSITION, FOCUS; negation FORM/USE: 'NEITHER ... NOR'; superlatives FORM/USE: 'THE BEST' WITH NOUN AND PRESENT PERFECT\n",
      "\n",
      "Snippet:\n",
      "A: Yes it is, the 2012 Avengers made by Marvel Comics.\n",
      "B: I'd definitely watch that again. Who's your favorite character?\n",
      "A: It's Nick Fury, the director of S.H.I.E.L.D\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['If you could choose any superpower, what would it be?', 'If you could have any superpower, what would it be?', 'If you could choose any superpower, what would it be?']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaa107c001041f7b5cfcf01cb5bd1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.3939393939393939], 'positive_constraints': [[[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[2.0, 4.0, 2.0]], 'Relevance': [[2.0, 2.0, 2.0]], 'Content Richness': [[2.0, 3.0, 2.0]], 'Grammatical Correctness': [[4.0, 4.0, 4.0]]}\n",
      "\n",
      "B1-Can use 'would' in the main clause of a conditional sentence to talk about an imagined situation, often in the context of advice or opinion-giving. (625)\n",
      "- If you could choose any superpower, what would it be?\n",
      "\n",
      "B1-Can use question forms. (628)\n",
      "- If you could choose any superpower, what would it be?\n",
      "\n",
      "B1-Can use 'would' in the main clause of a conditional sentence to talk about an imagined situation, often in the context of advice or opinion-giving. (625)\n",
      "- If you could have any superpower, what would it be?\n",
      "\n",
      "B1-Can use question forms. (628)\n",
      "- If you could have any superpower, what would it be?\n",
      "\n",
      "B1-Can use 'would' in the main clause of a conditional sentence to talk about an imagined situation, often in the context of advice or opinion-giving. (625)\n",
      "- If you could choose any superpower, what would it be?\n",
      "\n",
      "B1-Can use question forms. (628)\n",
      "- If you could choose any superpower, what would it be?\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of at least one of these grammar skills:\n",
      "would FORM/USE: AFTER 'IF' CLAUSES; would FORM: PAST AFFIRMATIVE; would FORM: PAST NEGATIVE; would FORM: QUESTIONS; would FORM: WITH ADVERBS; would USE: FUTURE IN THE PAST; would USE: IMAGINED SITUATIONS IN THE PAST; would USE: INDIRECTNESS; would USE: POLITE REQUESTS; would USE: REPORTED SPEECH; would USE: WILLINGNESS IN THE PAST; negation FORM/USE: 'NOT', EMPHASIS; negation FORM/USE: 'NEVER', INVERTED FRONT POSITION, FOCUS; negation FORM/USE: 'NEITHER ... NOR'; superlatives FORM/USE: 'THE BEST' WITH NOUN AND PRESENT PERFECT\n",
      "\n",
      "Snippet:\n",
      "A: I love batman\n",
      "B: He is so mysterious \n",
      "A: Sorta ironic that they become enemies \n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "[\"Yeah, if Batman hadn't witnessed his parents' murder, he wouldn't have become the Dark Knight.\", 'If Batman had never become enemies with the Joker, Gotham City would probably be a much safer place.', \" If they had known each other's identities, maybe they wouldn't have been enemies.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5717674c7b44119f2e9ffc8d31af25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.9454545454545454], 'positive_constraints': [[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 2.0, 4.0]], 'Relevance': [[4.0, 2.0, 4.0]], 'Content Richness': [[4.0, 4.0, 4.0]], 'Grammatical Correctness': [[5.0, 5.0, 5.0]]}\n",
      "\n",
      "B1-Can use 'would' in the main clause of a conditional sentence to talk about an imagined situation, often in the context of advice or opinion-giving. (625)\n",
      "- Yeah, if Batman hadn't witnessed his parents' murder, he wouldn't have become the Dark Knight.\n",
      "\n",
      "B1-Can use 'would not have' + '-ed' or 'wouldn’t have' + '-ed' (627)\n",
      "- Yeah, if Batman hadn't witnessed his parents' murder, he wouldn't have become the Dark Knight.\n",
      "\n",
      "B1-Can use 'would' to talk about imagined situations in the past. ► conditionals (631)\n",
      "- Yeah, if Batman hadn't witnessed his parents' murder, he wouldn't have become the Dark Knight.\n",
      "\n",
      "B1-Can use 'would' with verbs such as 'advise', 'imagine', 'recommend', 'say' to be less direct. (632)\n",
      "- Yeah, if Batman hadn't witnessed his parents' murder, he wouldn't have become the Dark Knight.\n",
      "\n",
      "B1-Can use 'would' in the main clause of a conditional sentence to talk about an imagined situation, often in the context of advice or opinion-giving. (625)\n",
      "- If Batman had never become enemies with the Joker, Gotham City would probably be a much safer place.\n",
      "\n",
      "B1-Can use an limited range of adverbs with 'would', including 'really', 'probably', 'certainly', 'definitely'.► adverbs (629)\n",
      "- If Batman had never become enemies with the Joker, Gotham City would probably be a much safer place.\n",
      "\n",
      "B1-Can use 'would' in the main clause of a conditional sentence to talk about an imagined situation, often in the context of advice or opinion-giving. (625)\n",
      "-  If they had known each other's identities, maybe they wouldn't have been enemies.\n",
      "\n",
      "B1-Can use 'would have' + '-ed'. (626)\n",
      "-  If they had known each other's identities, maybe they wouldn't have been enemies.\n",
      "\n",
      "B1-Can use 'would not have' + '-ed' or 'wouldn’t have' + '-ed' (627)\n",
      "-  If they had known each other's identities, maybe they wouldn't have been enemies.\n",
      "\n",
      "B1-Can use 'would' to talk about imagined situations in the past. ► conditionals (631)\n",
      "-  If they had known each other's identities, maybe they wouldn't have been enemies.\n",
      "\n",
      "B1-Can use 'would' with verbs such as 'advise', 'imagine', 'recommend', 'say' to be less direct. (632)\n",
      "-  If they had known each other's identities, maybe they wouldn't have been enemies.\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rules = sample_subcategory_rules()\n",
    "    snippet, _ = sample_dialog_snippet(dialogs)\n",
    "    prompt_score(rules, snippet, classifiers, dialog=True, all=False, verbose=True, statement=False, n_responses=3)\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3140d76-fb71-48a3-afc3-55abf35a3c55",
   "metadata": {},
   "source": [
    "Subcategory constraints parameters\n",
    "1. number of subcategories to choose constraints from -> 1-3 (cost of developing more classifiers)\n",
    "2. CEFR levels -> A1-C2\n",
    "3. maximum difference between CEFR levels per category\n",
    "\n",
    "dataset format\n",
    "\n",
    "CTX_SOURCE_DS | CTX_SOURCE_ID | CONTEXT | RESPONSE | SUBCATS x LEVELS\n",
    "\n",
    "Daily Dialog | 023 | A: Hey B: Hey A: How are you? | B: I'm good | would_B2, negation_C1, superlatives_B1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597ecdf-abfe-4998-bd60-5513dbf9c5a4",
   "metadata": {},
   "source": [
    "## Task 3: All skills from one difficulty level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ee1add-a19d-4fd9-a421-22fbc905cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_level_rules(level = None):\n",
    "    if level is None: level = random.sample(levels, 1)[0]\n",
    "    return level, egp[(egp['Level']==level) & (egp['#'].isin(classifiers.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44f00b26-1705-40f6-ac77-9dda35c24a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Continue the text with two sentences, proving knowledge of grammar skills on CEFR level A1.\n",
      "Snippet:\n",
      "President Obama will place tariffs on imports of some Chinese tires for three years in an effort to curb a surge in exports that has rocked the U.S. tire industry.\n",
      "RESPONSES from GPT3.5\n",
      "['The decision has come under criticism from China, sparking concerns of a potential trade war between the two countries. Nevertheless, the President believes this action is necessary to protect American jobs and industries.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level, rules = sample_level_rules()\n",
    "snippet = random.sample(stories, 1)[0]\n",
    "prompt_score(rules, snippet, classifiers, dialog=False, all=False, verbose=True, level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c007342-a2f8-4bb9-9464-be3525580c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Continue the dialog with one turn, proving knowledge of grammar skills on CEFR level C1.\n",
      "Snippet:\n",
      "A: Steve Carrell is the main character with Jason Segel and Russell Brand. \n",
      "B: Gosh, I bet it is funny then.  Is it a Pixar movie?\n",
      "A: No, it actually is produced by Universal and Illumination.\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['Illumination, the studio responsible for hits such as \"Despicable Me\" and \"The Secret Life of Pets.\"', \"Oh, I see. So it's not animated then, but more of a live-action comedy film?\", \"Ah, I see. I didn't know that. Thank you for clarifying.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7367fe10af2404a988125f1e0ee6f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.9454545454545454], 'positive_constraints': [[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[4.0, 4.0, 4.0]], 'Content Richness': [[4.0, 4.0, 3.0]], 'Grammatical Correctness': [[5.0, 5.0, 5.0]]}\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of grammar skills on CEFR level A2.\n",
      "Snippet:\n",
      "A: Do you like animated\n",
      " movies?\n",
      "B: anyone there?\n",
      "A: hai\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['Yes, I like animated movies.', 'Yes, I enjoy watching animated movies.', 'Do you like animated movies too?']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a63d13e55241e782dc4d944fba11b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.6842105263157895], 'positive_constraints': [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[2.0, 2.0, 2.0]], 'Relevance': [[2.0, 4.0, 2.0]], 'Content Richness': [[2.0, 2.0, 2.0]], 'Grammatical Correctness': [[4.0, 4.0, 4.0]]}\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of grammar skills on CEFR level B2.\n",
      "Snippet:\n",
      "A: yes  i was band play\n",
      "B: i like the music\n",
      "A: When the band  starts their tour I found it interesting that Mia accuses Sebastian of abandoning hie dreams!\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['Is she just jealous of his success?', \"It's true that Mia accuses Sebastian of abandoning his dreams, but ultimately they both realize the importance of pursuing their passions.\", 'Yes, it was a pivotal moment in the movie when Mia confronts Sebastian about his priorities.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86407c263e6f4609807e26b17d7c4764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [1.0], 'positive_constraints': [[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[4.0, 4.0, 5.0]], 'Content Richness': [[3.0, 4.0, 4.0]], 'Grammatical Correctness': [[4.0, 4.0, 5.0]]}\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of grammar skills on CEFR level B1.\n",
      "Snippet:\n",
      "A: Wow, that's awesome.  Is there a love story between Turing and the character played by Knightley?\n",
      "B: Yes, Turing proposes to her, but it is revealed that he is actually a homosexual.\n",
      "A: Quite a twist\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['Indeed, it adds a layer of complexity to his character and the overall plot.', 'Yes, it adds a layer of complexity to the story and sheds light on the challenges Turing faced during that time period.', 'Yes, it certainly adds an interesting layer to the story.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f3e133acc1478db4fec36b987a2e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.7755102040816326], 'positive_constraints': [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[4.0, 4.0, 4.0]], 'Content Richness': [[4.0, 4.0, 3.0]], 'Grammatical Correctness': [[5.0, 5.0, 5.0]]}\n",
      "____________________________________________________________________________________________________\n",
      "PROMPT: Continue the dialog with one turn, proving knowledge of grammar skills on CEFR level B1.\n",
      "Snippet:\n",
      "A: I really like the movie Zootopia. \n",
      "B: The Rotten Tomatoes rating is 98%. That is a really good rating for a movie.\n",
      "A: zootopia yeah that was good\n",
      "B: \n",
      "RESPONSES from GPT3.5\n",
      "['Yes, I thought the animation was fantastic and the storyline was very entertaining.', 'I agree, the animation was impressive and the storyline was engaging.', 'It was not only entertaining but also had a good message about equality and acceptance.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74278d90a42a48a4868af9f4e4a8e494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Contexts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Distinctiveness': [0.8780487804878049], 'positive_constraints': [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], 'negative_constraints': [[[], [], []]], 'Appropriateness': [[4.0, 4.0, 4.0]], 'Relevance': [[4.0, 4.0, 5.0]], 'Content Richness': [[4.0, 4.0, 4.0]], 'Grammatical Correctness': [[5.0, 5.0, 5.0]]}\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    level, rules = sample_level_rules()\n",
    "    snippet, _ = sample_dialog_snippet(dialogs)\n",
    "    prompt_score(rules, snippet, classifiers, dialog=True, all=False, verbose=True, level=level, n_responses=3)\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae01db-7e57-415e-9809-bb57beff5842",
   "metadata": {},
   "source": [
    "Difficulty level constraints parameters\n",
    "1. CEFR levels -> A1-C2\n",
    "\n",
    "dataset format\n",
    "\n",
    "CTX_SOURCE_DS | CTX_SOURCE_ID | CONTEXT | RESPONSE | LEVEL\n",
    "\n",
    "Daily Dialog | 023 | A: Hey B: Hey A: How are you? | B: I'm good | B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d8a74-0672-4b7d-bc43-26c91a4868f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c32296-73fb-4cb5-a6ad-0a26a59be04c",
   "metadata": {},
   "source": [
    "# Exp007: Train grammar rule classifier\n",
    "This experiment elaborates different ways to train a binary classifier to detect the usage of an EGP rule in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87ba234-c82d-4a00-ba5e-1c9e0bb2f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_local/mpb672-5218874/tmp/ipykernel_84349/1039383077.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/mnt/qb/work/meurers/mpb672/conda_envs/gctg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5e66d3b-cfe0-4c90-8a64-6cb8568cfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleDetector(torch.nn.Module):\n",
    "    def __init__(self, bert_encoder, hidden_dim=32, dropout_rate=0.25, train_bert=False):\n",
    "        super().__init__()\n",
    "        self.bert = bert_encoder\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = train_bert\n",
    "        input_dim = self.bert.config.hidden_size*(self.bert.config.num_hidden_layers+1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.hidden = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, diagnose=False):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids, attention_mask)\n",
    "            x = torch.cat(outputs.hidden_states, dim=-1)\n",
    "        if diagnose:\n",
    "            print(x.shape)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(-1).expand(x.size())\n",
    "        x *= extended_attention_mask.float()  # Convert mask to float and apply\n",
    "        x = self.dropout(x)\n",
    "        x = self.hidden(x)\n",
    "        if diagnose:\n",
    "            print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        if diagnose:\n",
    "            print(x.shape)\n",
    "        \n",
    "        max_values, max_indices = torch.max(x, 1)\n",
    "        return max_values.flatten(), max_indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0186655-581e-4419-ba2a-9fca2c2761d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.sentences[idx], return_tensors='pt', max_length=self.max_len, padding='max_length')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "def get_dataset(row, tokenizer, max_len, df, random_negatives=True, ratio = 0.5, max_positive_examples=500):\n",
    "    # assemble dataset for one construction\n",
    "    # 50% positive examples\n",
    "    unique_examples = list(set(row['augmented_examples']))\n",
    "    sentences = unique_examples[:max_positive_examples]\n",
    "    labels = [1] * len(sentences)\n",
    "\n",
    "    num_augs = int(len(sentences) * (1-ratio)) if random_negatives else len(sentences)\n",
    "    # augmented negative examples\n",
    "    aug_neg_examples = list(set(row['augmented_negative_examples']).difference(set(row['augmented_examples'])))\n",
    "    random.shuffle(aug_neg_examples)\n",
    "    unique_negatives = aug_neg_examples[:num_augs]\n",
    "    sentences += unique_negatives\n",
    "    labels += [0] * len(unique_negatives)\n",
    "    \n",
    "    if random_negatives:\n",
    "        num_rands = max_positive_examples - len(unique_negatives) # fill to an even number\n",
    "        # rest: random negative examples (positive from other constructions)\n",
    "        neg_examples = [example for sublist in df.loc[df['#'] != row['#'], 'augmented_examples'].to_list() for example in sublist]\n",
    "        random.shuffle(neg_examples)\n",
    "        sentences += neg_examples[:num_rands]\n",
    "        labels += [0] * len(neg_examples[:num_rands])\n",
    "    assert len(sentences) == 2 * max_positive_examples\n",
    "    assert sum(labels) == max_positive_examples\n",
    "    return SentenceDataset(sentences, labels, tokenizer, max_len)\n",
    "\n",
    "def get_loaders(dataset, batch_size=16):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = total_size - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "44da9976-2633-4816-9048-b85fdf4fdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir=os.getenv('CACHE_DIR'))\n",
    "bert_encoder = BertModel.from_pretrained('bert-base-uncased', cache_dir=os.getenv('CACHE_DIR'), output_hidden_states=True)\n",
    "classifier = RuleDetector(bert_encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843e27fd-c6c0-42d6-b14e-30e382274478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 319553\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in classifier.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a2f87-7b37-425a-beae-3f34f5be5bdf",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0196c0e-e5b7-4bd4-ba7f-72b90fb558a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5706], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Switzerland is a beautiful country.\"\n",
    "encoded_input = bert_tokenizer(input_text, return_tensors='pt').to(device)\n",
    "output = classifier(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
    "output['max_values']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050fd0d3-3545-4d9d-9811-d25c9d66f315",
   "metadata": {},
   "source": [
    "Train the rule detector for one random rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9ef98f-1b37-4b61-a606-2e2c15ac69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "egp_examples = pd.read_json(\"../data/egp_examples.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce1c2f26-fadd-4885-94f4-9b57d9d35574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can use a limited range of expressions with 'be' + infinitive ('be allowed to', 'be supposed to', 'be able to') with present and past forms of 'be' and with modal 'will'. \n",
      "First of all, if you are allowed to go out of the building in your break, you should do it. \n",
      "\n",
      "Perhaps you will be allowed to go on holiday with your friends next year. \n",
      "\n",
      "The film is supposed to start at 7.00 pm so we'd better meet at 6.30 pm. See you there! \n",
      "\n",
      "I was supposed to be meeting my friend Laura but she didn't come. \n",
      "\n",
      "I am sorry but I  am not able to meet you next Tuesday. \n",
      "\n",
      "We were able to choose the songs ourselves and so I liked them very much.\n",
      "['I am supposed to call my grandmother to wish her a happy birthday.', 'The team was able to win the game despite the tough competition.', 'Were they allowed to study together for the test?', 'We are allowed to bring our own snacks to the movie theater.', 'The guests are supposed to arrive at the party by 8 pm.', 'I think she will be able to finish the marathon if she keeps training.', 'She is supposed to be going on a business trip next month.', 'They were supposed to visit us last summer, but something came up.', \"The concert is supposed to start at 8 pm, so don't be late.\", 'We will be allowed to use the gym after school.']\n",
      "['They arrive at the airport just in time for their flight.', 'We solved the math problem without any help from the teacher.', 'We finished the project by the deadline.', 'We visit the museum during our trip to the city.', 'He overcame his fear of public speaking with practice and determination.', 'The children played in the garden after they finished their homework.', 'The students leave the classroom early because of the heat.', 'Were you able to find your keys in the end?', 'She speaks three different languages fluently.', 'The children watch TV after finishing their homework.']\n"
     ]
    }
   ],
   "source": [
    "rule = egp_examples.sample(1).iloc[0]\n",
    "print(rule['Can-do statement'])\n",
    "print(rule['Example'])\n",
    "print(random.sample(rule['augmented_examples'], 10))\n",
    "print(random.sample(rule['augmented_negative_examples'], 10))\n",
    "classifier = RuleDetector(bert_encoder).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), 1e-4)\n",
    "dataset = get_dataset(rule, bert_tokenizer, 64, egp_examples) \n",
    "train_dataloader, val_dataloader = get_loaders(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "868049f6-9dbd-4bf4-baab-945b5c457f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RuleDetector(bert_encoder).to(device)\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "90a2f2f7-0ad2-41da-8035-34bf5aa3499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 35.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4804320377111435\n",
      "Accuracy: 0.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 36.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.35990911960601807\n",
      "Accuracy: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 37.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.35357400119304655\n",
      "Accuracy: 0.995\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, val_dataloader, num_epochs=3, criterion = torch.nn.BCELoss()):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            values, _ = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(values, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f'Training loss: {avg_train_loss}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval() \n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        \n",
    "        with torch.no_grad():  # No gradients needed for validation\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "    \n",
    "                outputs, _ = model(input_ids, attention_mask)\n",
    "                predictions = outputs > 0.5                \n",
    "                total_correct += (predictions.flatten() == labels).sum().item()\n",
    "                total_examples += labels.size(0)\n",
    "\n",
    "        accuracy = total_correct / total_examples\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        \n",
    "train(classifier, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fa8e3598-66a6-46f4-b6c2-581e01a93017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 9984])\n",
      "torch.Size([1, 64, 32])\n",
      "torch.Size([1, 64, 1])\n",
      "Score: 0.4964766800403595\n",
      "Maximum token: [PAD]\n"
     ]
    }
   ],
   "source": [
    "input_text = 'I want to call my grandmother to wish her a happy birthday'\n",
    "encoded_input = bert_tokenizer(input_text, return_tensors='pt', max_length=64, padding='max_length').to(device)\n",
    "with torch.no_grad():\n",
    "    values, indices = classifier(encoded_input['input_ids'], encoded_input['attention_mask'], diagnose=True)\n",
    "    print(f'Score: {values.item()}')\n",
    "tokens = bert_tokenizer.convert_ids_to_tokens(encoded_input['input_ids'].squeeze().tolist())\n",
    "print(f'Maximum token: {tokens[indices]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "983e919e-a39f-47f9-bf73-d10755d8434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 1),\n",
       " (1045, 1),\n",
       " (2089, 1),\n",
       " (2036, 1),\n",
       " (2272, 1),\n",
       " (4826, 1),\n",
       " (1012, 1),\n",
       " (102, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(encoded_input['input_ids'][0].cpu().tolist(), encoded_input['attention_mask'][0].cpu().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fb678-4cbb-4cab-b2d4-c8d00d92e099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15365f89-66d7-45e1-ad97-dc4610aeeeb5",
   "metadata": {},
   "source": [
    "# Exp018: Conditional instruction fine-tuning\n",
    "This experiment aims at instruction fine-tuning from existing skills in the dataset to train the model on single constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f05147-1c67-4b51-b62b-139c888f833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 18:06:12.305290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(f'../source')\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d860d9f7-cfd7-4c60-8ab7-ac59fa12d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "out_file = '../data/corpus_classification.pkl'\n",
    "preprossed_dataset_file = '../data/SFT_data.jsonl'\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "nrs = [616] #[58, 616]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcfa5faf-3b92-4ea9-a00b-36a0086adbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "egp = helpers.get_egp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f860be6b-ba7d-498f-9147-f65d5d8b1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_file, 'rb') as f:\n",
    "    all_hit_indices = pickle.load(f)\n",
    "    all_hit_sentences = pickle.load(f)\n",
    "    extracts = pickle.load(f)\n",
    "\n",
    "data = [{\"context\": extracts[idx][0], \"response\": extracts[idx][1], \"nr\": nr} for nr in nrs for idx in all_hit_indices[nr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eebd3e-f3da-4d9d-89c4-92bccf50d23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd585630de745a4ba9f2ff380da2e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/741 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    rules = egp[egp['#'].isin(example['nr'] if type(example['nr']) == list else [example['nr']])]\n",
    "    constraints = os.linesep.join(\"- \" + rules['SubCategory'] + \" - \" + rules['guideword'] + \": \" + rules['Can-do statement'])\n",
    "    context = os.linesep.join([(\"A\" if (i%2==0) else \"B\") + \": \" + utt for i, utt in enumerate(example[\"context\"])])\n",
    "\n",
    "    return f\"\"\"[INST] Continue the dialog with one turn and show all of these grammar skills in your response.\n",
    "Grammar skills:\n",
    "{constraints}\n",
    "Dialog:\n",
    "{context} [/INST] \n",
    "A: {example['response']}</s>\"\"\"\n",
    "    \n",
    "with open(preprossed_dataset_file, 'w') as f:\n",
    "    for item in tqdm(data):\n",
    "        # line['prompt'], line['completion'] = formatting_func(item) # for completion chat format\n",
    "        item['text']  = formatting_func(item)\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb69f077-228e-4054-95a7-9bd8c359bd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240456a128ae4ba9ab7846762b9f0c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files=preprossed_dataset_file, split='train', cache_dir=os.getenv('CACHE_DIR'))\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d5b6272-7374-4273-8882-e506bc647845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['What is the usual thing here?',\n",
       "  'Forty to eighty RMB Yuan a plate.',\n",
       "  \"Let's say sixty Yuan then.\",\n",
       "  \"OK. Is there anything special you'd like to have on the menu?\"],\n",
       " 'response': \"We'd like to have typical Chinese food.\",\n",
       " 'nr': 616,\n",
       " 'text': \"[INST] Continue the dialog with one turn and show all of these grammar skills in your response.\\nGrammar skills:\\n- would - FORM: AFFIRMATIVE WITH 'LIKE': Can use the affirmative form with 'like'. \\nDialog:\\nA: What is the usual thing here?\\nB: Forty to eighty RMB Yuan a plate.\\nA: Let's say sixty Yuan then.\\nB: OK. Is there anything special you'd like to have on the menu? [/INST] \\nA: We'd like to have typical Chinese food.</s>\"}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b489244c-bcab-4da7-ad02-abbce1048030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20abf5bc061a465da0a7b9fc392a6585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, cache_dir=os.getenv('CACHE_DIR'), device_map=\"auto\")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, cache_dir=os.getenv('CACHE_DIR'))\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a331450b-9ac4-4a40-9e2a-2758706a076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dad, what do you want to have? I'd like a hamburger.\n",
      "[INST] Continue the dialog with one turn and show all of these grammar skills in your response.\n",
      "Grammar skills:\n",
      "- would - FORM: AFFIRMATIVE WITH 'LIKE': Can use the affirmative form with 'like'. \n",
      "Dialog:\n",
      "A: I'd like to go to McDonald's this time.\n",
      "B: OK then. You'll drive, will you?\n",
      "A: No, I'm a bit tired today. You do that, please.\n",
      "B: OK, OK. I always do things like that ... Ah, here we are. [/INST] \n",
      "A: Do you want anything from there or not ?\n"
     ]
    }
   ],
   "source": [
    "example = random.choice(train_dataset)\n",
    "#example['nr'] = [58, 616]\n",
    "#example['text'] = formatting_func(example)\n",
    "print(example['response'])\n",
    "\n",
    "#converted_sample = [\n",
    "#    {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "#    #{\"role\": \"assistant\", \"content\": example[\"completion\"]},\n",
    "#]\n",
    "#model_input = tokenizer.apply_chat_template(converted_sample, return_tensors=\"pt\").to(device)\n",
    "\n",
    "EOP = \"[/INST]\"\n",
    "eval_prompt = example['text'][:example['text'].index(EOP)+len(EOP)]\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_code = tokenizer.decode(model.generate(**model_input, max_new_tokens=1024, pad_token_id=2, repetition_penalty=1.5)[0], skip_special_tokens=True)\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "125626b6-a4ab-41eb-8940-fa8fa0745d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b8ee436-02f3-47b9-b605-830bff043f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../models/mistral_FT\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    #save_steps=25,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=250,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"gctg\",\n",
    "    #load_best_model_at_end=True,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps=25,\n",
    "    #per_device_eval_batch_size=32,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa48307e-a483-404f-bb12-170ef022ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=256,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    #neftune_noise_alpha=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a8ea4-8027-4101-8a7e-fbfa620066d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79/250 03:06 < 06:54, 0.41 it/s, Epoch 0.53/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.523900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.468100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250aaa3-02de-4794-ba53-19d3e0c233d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gctg)",
   "language": "python",
   "name": "gctg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

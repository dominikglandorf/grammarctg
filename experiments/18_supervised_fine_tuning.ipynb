{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15365f89-66d7-45e1-ad97-dc4610aeeeb5",
   "metadata": {},
   "source": [
    "# Exp018: Conditional instruction fine-tuning\n",
    "This experiment aims at instruction fine-tuning from existing skills in the dataset to train the model on single constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c19775e-4db4-4d58-9310-0e82d15f32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 17:47:32.848301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/cluster/home/dglandorf/grammarctg/experiments/../source/models.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "\n",
    "\n",
    "import pickle\n",
    "from torch.utils.data import RandomSampler\n",
    "import numpy as np\n",
    "import json\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(f'../source')\n",
    "import helpers\n",
    "import models\n",
    "import importlib\n",
    "#importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d860d9f7-cfd7-4c60-8ab7-ac59fa12d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "out_file = '../data/corpus_classification.pkl'\n",
    "preprossed_dataset_file = '../data/SFT_data.jsonl'\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "nrs = [616] #[58, 616]#\n",
    "classifier = models.load_classifier(616, \"corpus_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcfa5faf-3b92-4ea9-a00b-36a0086adbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "egp = helpers.get_egp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bf8eb-e0b0-4d72-92de-39d59362eafa",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860be6b-ba7d-498f-9147-f65d5d8b1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_file, 'rb') as f:\n",
    "    all_hit_indices = pickle.load(f)\n",
    "    all_hit_sentences = pickle.load(f)\n",
    "    extracts = pickle.load(f)\n",
    "\n",
    "data = [{\"context\": extracts[idx][0], \"response\": extracts[idx][1], \"nr\": nr} for nr in nrs for idx in all_hit_indices[nr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eebd3e-f3da-4d9d-89c4-92bccf50d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    rules = egp[egp['#'].isin(example['nr'] if type(example['nr']) == list else [example['nr']])]\n",
    "    constraints = os.linesep.join(\"- \" + rules['SubCategory'] + \" - \" + rules['guideword'] + \": \" + rules['Can-do statement'])\n",
    "    context = os.linesep.join([(\"A\" if (i%2==0) else \"B\") + \": \" + utt for i, utt in enumerate(example[\"context\"])])\n",
    "\n",
    "    return f\"\"\"[INST] Write an answer of A that includes the affirmative form of \"would like\".\n",
    "\n",
    "Dialog:\n",
    "{context} [/INST] \n",
    "A: {example['response']}</s>\"\"\"\n",
    "    \n",
    "    return f\"\"\"[INST] Continue the dialog with one turn and show all of these grammar skills in your response.\n",
    "Grammar skills:\n",
    "{constraints}\n",
    "Dialog:\n",
    "{context} [/INST] \n",
    "A: {example['response']}</s>\"\"\"\n",
    "\n",
    "with open(preprossed_dataset_file, 'w') as f:\n",
    "    for item in tqdm(data[:500]):\n",
    "        # line['prompt'], line['completion'] = formatting_func(item) # for completion chat format\n",
    "        item['text']  = formatting_func(item)\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2f6bc-a141-4832-8f94-af0732f0b8da",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb69f077-228e-4054-95a7-9bd8c359bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=preprossed_dataset_file, split='train', cache_dir=os.getenv('CACHE_DIR'))\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset, test_dataset = train_test_split['train'], train_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e897370-34b9-4776-9d6c-5eaa66f24db6",
   "metadata": {},
   "source": [
    "## Load and prepare base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b489244c-bcab-4da7-ad02-abbce1048030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c7ce2632864017a671d006834432f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, cache_dir=os.getenv('CACHE_DIR'), device_map=\"auto\")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, cache_dir=os.getenv('CACHE_DIR'))\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302a546-0eff-452a-b195-dbe6ceede8a9",
   "metadata": {},
   "source": [
    "### Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a331450b-9ac4-4a40-9e2a-2758706a076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Write an answer of A that includes the affirmative form of \"would like\".\n",
      "\n",
      "Dialog:\n",
      "A: Good morning, sir. May I come in?\n",
      "B: Good morning. Yes, please. Take a seat. I guess you want to open an account, right?\n",
      "A: Yes.\n",
      "B: Great. What account do you want to open? A checking account or a saving account? [/INST] \n",
      "A: I'd like to open a saving account.\n"
     ]
    }
   ],
   "source": [
    "example = random.choice(test_dataset)\n",
    "#example = train_dataset[10]\n",
    "#example['nr'] = [58, 616]\n",
    "#example['text'] = formatting_func(example)\n",
    "#print(example['text'])\n",
    "\n",
    "#converted_sample = [\n",
    "#    {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "#    #{\"role\": \"assistant\", \"content\": example[\"completion\"]},\n",
    "#]\n",
    "#model_input = tokenizer.apply_chat_template(converted_sample, return_tensors=\"pt\").to(device)\n",
    "\n",
    "EOP = \"[/INST]\"\n",
    "eval_prompt = example['text'][:example['text'].index(EOP)+len(EOP)+4]\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    token_ids = model.generate(**model_input, max_new_tokens=1024, pad_token_id=2)[0]\n",
    "    output_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b8ed6-4a6a-4237-acc5-e0633d076a4b",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125626b6-a4ab-41eb-8940-fa8fa0745d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8ee436-02f3-47b9-b605-830bff043f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../models/mistral_FT\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    #save_steps=25,\n",
    "    logging_steps=20,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=250,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"gctg\",\n",
    "    #load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    per_device_eval_batch_size=20,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f259bf-cf93-434c-b568-2d2064a3289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"[/INST]\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f788de-4d48-497b-99fd-add052918ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, device=\"cpu\")\n",
    "    print(decoded_preds)\n",
    "    #decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, device=\"cpu\")\n",
    "    #rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "\n",
    "    \n",
    "def compute_metrics(eval_preds):\n",
    "    #preds, labels = eval_preds\n",
    "    #print(preds, labels)\n",
    "    print(\"EPOCH\", \"___\" * 20)\n",
    "    for ds in [train_dataset, test_dataset]:\n",
    "        random_sampler = RandomSampler(ds, num_samples=3)\n",
    "        for idx in random_sampler:\n",
    "            text = ds[idx]['text']\n",
    "            #print(text)\n",
    "            EOP = \"[/INST]\"\n",
    "            response_idx = text.index(EOP)+len(EOP)+4\n",
    "            eval_prompt = text[:response_idx]\n",
    "            model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                token_ids = model.generate(**model_input, max_new_tokens=1024, pad_token_id=2, repetition_penalty=1.5)[0]\n",
    "                output_text = tokenizer.decode(token_ids, skip_special_tokens=True, device=\"cpu\")\n",
    "            print(f\"Truth: {text[response_idx:]}\")\n",
    "            print(f\"Generated: {output_text[response_idx:]}\")\n",
    "            print(f\"Grammar score: {models.probe_model(classifier, [str(output_text[response_idx:])])[0].item()}\")\n",
    "    return {\"metric\": 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa48307e-a483-404f-bb12-170ef022ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2986577bb3914747b1c8c24480932a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cff4fa934345739ca18a9966885e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/dglandorf/gctg/lib64/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    data_collator=collator,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    "    #neftune_noise_alpha=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255a8ea4-8027-4101-8a7e-fbfa620066d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdo-gl\u001b[0m (\u001b[33mdomgla\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cluster/home/dglandorf/grammarctg/experiments/wandb/run-20240404_174909-msvym0f5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/domgla/huggingface/runs/msvym0f5' target=\"_blank\">gctg</a></strong> to <a href='https://wandb.ai/domgla/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/domgla/huggingface' target=\"_blank\">https://wandb.ai/domgla/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/domgla/huggingface/runs/msvym0f5' target=\"_blank\">https://wandb.ai/domgla/huggingface/runs/msvym0f5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 05:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.193800</td>\n",
       "      <td>1.811004</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.673900</td>\n",
       "      <td>1.791012</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.349800</td>\n",
       "      <td>1.870736</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.035900</td>\n",
       "      <td>1.907026</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>1.986304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>2.161965</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>2.326213</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>2.784266</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.124300</td>\n",
       "      <td>2.737197</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>2.770288</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>2.890385</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>2.780075</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ____________________________________________________________\n",
      "Truth:  Ugh, I don't think I would like that very much! I'm sure there are more interesting ways, maybe different types of steaks?</s>\n",
      "Generated:  That reminds me...I would love for someone else in our family to make dinner tonight so we could have something different than what mom makes all the time\n",
      "Grammar score: 0.014691396616399288\n",
      "Truth:  Yeah I don't think I would like it but I bet the Canadian bands sure do. </s>\n",
      "Generated:  Yeah but you would still have a lot more exposure if your songs were played in Canada than not being heard there\n",
      "Grammar score: 0.004068496637046337\n",
      "Truth:  Sure. We also would like to use the chairs from the lunchroom. Would that be OK?</s>\n",
      "Generated:  Yes we would prefer if someone was there when we drop off the car as well\n",
      "Grammar score: 0.1942264884710312\n",
      "Truth:  I would like to watch it again</s>\n",
      "Generated:  Yeah, definitely!\n",
      "Grammar score: 0.0016454245196655393\n",
      "Truth:  Well i have the best you might like. the tesla the one that Elon Must is a CEO of</s>\n",
      "Generated:  How do you feel about Honda vehicles, they are known for their reliability\n",
      "Grammar score: 0.0017673687543720007\n",
      "Truth:  net really, no. we'd preferably like to live in a quiet street.</s>\n",
      "Generated:  well...we would prefer not living with students if at all possible!\n",
      "Grammar score: 0.10660382360219955\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  why not? I would like to buy some candies. Since there are so many people, I think they must have delicious candy.</s>\n",
      "Generated:  Yes, we would still like to try their sweets because they are famous for them in this area.\n",
      "Grammar score: 0.9117794632911682\n",
      "Truth:  I don't think that eventually everyone will subscribe unless broadcast television stations like NBC, ABC, FOX, and CBS go away.  I don't see that happening.  I do wish I could get faster internet service though, I would like to be a subscriber.</s>\n",
      "Generated:  It seems they would need more servers for their data centers if this trend continues at such high rates.  \n",
      "Grammar score: 0.006753303110599518\n",
      "Truth:  I would like to visit DC and see all of our government in action.  Did you know that we didn't use colors to describe the political parties until 2000?  That was so recent.</s>\n",
      "Generated:  Yeah i would really love to visit DC and see all these things for myself\n",
      "Grammar score: 0.06607002019882202\n",
      "Truth:  I know!  The funny thing is I've never heard of anything odd being used before.  I would like to see some film footage of early games to see if they used strange items.</s>\n",
      "Generated:  Haha! Yeah it is crazy how they used whatever was available back then\n",
      "Grammar score: 0.0017056592041626573\n",
      "Truth:  Oh, I see. Yeah. Some people like traveling, some don't. My husband didn't like traveling at first, either. He said what's the use of traveling? A waste of money and a waste of time. At last I got him on the move. After all, he loved me. We went to Bali, Indonesia, one of the most beautiful places in the world. That trip left a very deep impression on him. Since then whenever we have holidays and money, we'd like to patronize the travel agency. Isn't it a bit dull to shut yourself at one place all your life? </s>\n",
      "Generated:  That sounds nice! My parents would also love it if they could travel again nowadays but unfortunately their health is not good enough anymore and so are mine...I think i need some rest from work as well..\n",
      "Grammar score: 0.04642592370510101\n",
      "Truth:  I would like to have some tea as well.</s>\n",
      "Generated:  Yes please give us some mineral water and coffee as well.\n",
      "Grammar score: 0.009791810996830463\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  I do, it adds a sense of tenseness to the whole operation, where in the initial layers they might have a few seconds to spare, in the lower levels it could be hours, or even days. It's scary isn't it how Mal believed that she was still in a dream after leaving limbo, resulting in her killing herself to \"wake up\" in her mind.</s>\n",
      "Generated:  Yes i did really liked how they explained about the levels being based off our own personal experiences which makes me wonder what my first layer looks like lol\n",
      "Grammar score: 0.006080403458327055\n",
      "Truth:  I try to travel as much as possible. I'd like to go Iceland. In Iceland to go between cities you usually have to take domestic flights since they don't have public transportation.</s>\n",
      "Generated:  Not as often as i would like lol\n",
      "Grammar score: 0.9850281476974487\n",
      "Truth:  I'd like to buy 1 000 shares of Duson.</s>\n",
      "Generated:  Well, first let me know what stocks your company has for sale.\n",
      "Grammar score: 0.0056421468034386635\n",
      "Truth:  I'm a big fan of animated movies though, so I would like to watch this one. How about you?</s>\n",
      "Generated:  Maybe we should watch it together.\n",
      "Grammar score: 0.0030952589586377144\n",
      "Truth:  She wants to be able to contact you right away when she needs to borrow money. I would like to be able to make phone calls but not be pestered at all hours.</s>\n",
      "Generated:  Yeah i would still prefer landlines if we could get rid of all this technology\n",
      "Grammar score: 0.07799854129552841\n",
      "Truth:  I'd like to. But Bob, are you sure it will be all right? I would hate you to make your boss angry or even lose your job.</s>\n",
      "Generated:  Surely. Where would you like to have it?\n",
      "Grammar score: 0.41045108437538147\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  yes I would like to have that in real life lol</s>\n",
      "Generated:  oh ok, i would have liked to see more interaction between joseph gordon levy and scarlett Johansson\n",
      "Grammar score: 0.9189081788063049\n",
      "Truth:  I would like the extra lean.</s>\n",
      "Generated:  I'd prefer extra lean, but if they don't have it then regular is fine too.\n",
      "Grammar score: 0.2826695442199707\n",
      "Truth:  I'd like the egg drop soup.</s>\n",
      "Generated:  I'm not sure what type it is but i'll have a cup of tea if anyone else wants some coffee or milkshake they can let me know and then tell them how much money do we need to pay for this meal.\n",
      "Grammar score: 0.006318366155028343\n",
      "Truth:  I would like it transferred to my checking account.</s>\n",
      "Generated:  To a friend's checking account at another bank.\n",
      "Grammar score: 0.0024243593215942383\n",
      "Truth:  That's a good question. Apparently Unicef will donate a day of clean water to a child for every minute you do not use your cell phone. I would like to know how to get signed up for that program. </s>\n",
      "Generated:  Yeah i would have thought more insulation is needed to prevent interference from other signals in such vast amounts of water\n",
      "Grammar score: 0.11236266046762466\n",
      "Truth:  Yes, that's what it was about. I was really suprised at how the critics downplayed this movie. I thought they'd like it more.</s>\n",
      "Generated:  That sounds interesting! It seems to have received good reviews so i would recommend watching it if you get chance\n",
      "Grammar score: 0.13317330181598663\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  Right you are compadre!! I think i would like the Hawaiian version, with pork and pineapples with melted cheese on top.</s>\n",
      "Generated:  Yes but nowadays people would really appreciate this dish if someone made it just to please their guest\n",
      "Grammar score: 0.020284561440348625\n",
      "Truth:  I would just like to tour it, right? I did not know that the 3 wealthiest presidents were JFK, Washington, and Jefferson. Interesting...</s>\n",
      "Generated:  Yeah, it would probably have to accommodate quite a few people lol\n",
      "Grammar score: 0.012761715799570084\n",
      "Truth:  Yes, but it's downtown. I would like to take look at this one.</s>\n",
      "Generated:  I would really like this apartment if only...\n",
      "Grammar score: 0.7168717384338379\n",
      "Truth:  It that case, I'll send it by airmail. I'd like to send it as a registered parcel.</s>\n",
      "Generated:  Let me think a minute...I would still prefer not to pay extra for speed, but if you could send both ways at once--\n",
      "Grammar score: 0.26990821957588196\n",
      "Truth:  I imagine that could be true I would like to know more about that</s>\n",
      "Generated:  Yes! And also how ripe they were when picked\n",
      "Grammar score: 0.0021431257482618093\n",
      "Truth:  Yeah, sure. \"shark barking\" I would prefer a dog. Easier to pet, easier to take to the vet. Dogs are pretty smart.</s>\n",
      "Generated:  I would really love having one as a housepet, but they don't do well in small spaces and need lots of food!\n",
      "Grammar score: 0.10398904979228973\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  Um... I don't think so, but I'd like to be successful.</s>\n",
      "Generated:  Um...no, not particularly.But I think working harder than most other sales assistants might help me get promoted faster.And then after five or so years experience I'd like to start running one of your new superstores.\n",
      "Grammar score: 0.9971604347229004\n",
      "Truth:  I am definitely interested. I have only ever thought of ketone diets as either Atkins or Southbeach etc, so i would like to learn some more. Sounds good to me</s>\n",
      "Generated:  Yes thats what it was called on Southbeach Diet but they said you could eat more carbs if you wanted as long as they were from whole grains etc.. I think i would prefer this way because then at least there are some carbohydrates coming into my system\n",
      "Grammar score: 0.47161927819252014\n",
      "Truth:  Everybody attention. I would like to make sure you all know the process. The interview consists of three parts. One, all of the interviewees will answer the question there and lasts for maximum one hour. Two, we will take a 30 - minute's break. After the break, we all come back to this office and I will announce the successful candidates for the 2nd round. In which, you have a small interview with your future manager.</s>\n",
      "Generated:  Everybody attention! The questions are all about your skills and experience related with computer programming. Now let's start the questionnaire session. If anyone would like some explanation on a problem or need more time thinking before answering then just raise up your hand.\n",
      "Grammar score: 0.9774996638298035\n",
      "Truth:  Thanks. Please give me the style in this picture here but make the wave longer. I would like hair spray, please.</s>\n",
      "Generated:  Thank You. That one with long curly hair looks nice, but I don't think it suits me. Maybe if i had longer hair...I'd still like too though!\n",
      "Grammar score: 0.9363897442817688\n",
      "Truth:  Good for him! I would like to see him portraying the mouse king in a production of the nutcracker. </s>\n",
      "Generated:  That is so cool!! I think Busta Rhymes said something at his funeral which i would have liked to hear.\n",
      "Grammar score: 0.7339944243431091\n",
      "Truth:  I'd like a standard room.</s>\n",
      "Generated:  That sounds good. Then I ' d also like some breakfast in bed on my first day here.\n",
      "Grammar score: 0.9722306132316589\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  Great. Then I think this movie may be something that you would like. Ellen Page plays Ariadne a graduate student of architecture.</s>\n",
      "Generated:  Ellen Page who played Ariadne in Inception also stars in this film, so maybe i would like it after all!\n",
      "Grammar score: 0.9544815421104431\n",
      "Truth:  Let's stop at the newsagent, I would like to buy a newspaper. We can also buy postcards for your friends.</s>\n",
      "Generated:  Come on! Dad, let's not be late for school.\n",
      "Grammar score: 0.0016799945151433349\n",
      "Truth:  I didn't take that actress for the scientific type. I would like to attend one of Stanford's free AI classes.</s>\n",
      "Generated:  Who would have thought?! The future is scary but also exciting with advances such as brain computer interfaces which could allow us to type directly into our computers just by thinking about what we want typed!\n",
      "Grammar score: 0.008670233190059662\n",
      "Truth:  I'd like my deck to be at the same level as the back door.</s>\n",
      "Generated:  First off, it is for our new house in Lake Forest. We would like to build a small wooden dock at the end of the pier.\n",
      "Grammar score: 0.9963940978050232\n",
      "Truth:  I would like to pay by check.</s>\n",
      "Generated:  I'd like to use my credit card for payment.\n",
      "Grammar score: 0.9965906143188477\n",
      "Truth:  You're right. I'm not so sure about Korean rap though-they invented a compliment battle. Not sure if I'd like that.</s>\n",
      "Generated:  True! Now I'd like some more coffee before continuing this conversation...\n",
      "Grammar score: 0.9938017725944519\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  Right you are compadre!! I think i would like the Hawaiian version, with pork and pineapples with melted cheese on top.</s>\n",
      "Generated:  Right you are compadre!! i would like to try those myself one day..\n",
      "Grammar score: 0.9828872084617615\n",
      "Truth:  Not for sure except that it has something for everyone. It is the fourth largest populated urban area in the U.S. with 5.5 million people. I would like to retire and move there myself.</s>\n",
      "Generated:  Not for sure except it has something for everyone. It's the fourth largest populated urban area in the U.S with 5.6 million people.\n",
      "Grammar score: 0.004307483322918415\n",
      "Truth:  I have not. I don't know what it is about. I like all the Classics, though. I would like to write.</s>\n",
      "Generated:  I haven't but I want to. I've heard it is a good book about citizenship and responsibility.\n",
      "Grammar score: 0.0028992663137614727\n",
      "Truth:  I've listened to him for 12 years and it is getting too sportsy for me. And I do love sports! I would like more pop culture in there...</s>\n",
      "Generated:  Yeah me neither. I would just like some new voices.\n",
      "Grammar score: 0.9459941387176514\n",
      "Truth:  Good for him! I would like to see him portraying the mouse king in a production of the nutcracker. </s>\n",
      "Generated:  Wow, what a talent! I would have liked him if I had seen his movies.\n",
      "Grammar score: 0.22782756388187408\n",
      "Truth:  Yeah that is a rap battle that takes place in South Korea, would like to hear it sometime</s>\n",
      "Generated:  No but i would like to, is it similar to roasts or just compliments?\n",
      "Grammar score: 0.989020049571991\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  I would like to see him in more interesting roles.  He probably would make a good James Bond</s>\n",
      "Generated:  Yeah, it goes to show how versatile of an actress Scarlett Johansson is.   She played Black Widow in Iron Man and Avengers, but she would also make a great bond girl\n",
      "Grammar score: 0.006770681124180555\n",
      "Truth:  I'd like to go to an Italian restaurant.</s>\n",
      "Generated:  An Italian restaurant would be good.\n",
      "Grammar score: 0.021276654675602913\n",
      "Truth:  I think I would like this movie. Sounds interesting </s>\n",
      "Generated:  I think i would really like this movie\n",
      "Grammar score: 0.7589449286460876\n",
      "Truth:  net really, no. we'd preferably like to live in a quiet street.</s>\n",
      "Generated:  net really, no. i would preferably like it to be furnished, though.\n",
      "Grammar score: 0.925464391708374\n",
      "Truth:  Oh, I see. Yeah. Some people like traveling, some don't. My husband didn't like traveling at first, either. He said what's the use of traveling? A waste of money and a waste of time. At last I got him on the move. After all, he loved me. We went to Bali, Indonesia, one of the most beautiful places in the world. That trip left a very deep impression on him. Since then whenever we have holidays and money, we'd like to patronize the travel agency. Isn't it a bit dull to shut yourself at one place all your life? </s>\n",
      "Generated:  That sounds nice too. My father would like to go to Ireland one day. He has never left Asia before.\n",
      "Grammar score: 0.9887403845787048\n",
      "Truth:  I would like to know what that sounds like. There's also a website where you can listen to thousands of episodes of classic radio dramas for free.</s>\n",
      "Generated:  That was interesting. Now I would really like to hear some music.\n",
      "Grammar score: 0.9845386743545532\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  You too, is there anything else you would like to know before we finish the conversation. I think I need to tell you a little more before that happens.</s>\n",
      "Generated:  You too, Is there anything else you would like to know before we finish our conversation for now.\n",
      "Grammar score: 0.9243929386138916\n",
      "Truth:  I'd like to know more about it. I may want to see it, sometime in the future. </s>\n",
      "Generated:  I think i might need more information to decide if this movie would be something i would like\n",
      "Grammar score: 0.908105194568634\n",
      "Truth:  I like to surf in cold water behind the boat. Someday I would like to surf in the ocean as well. </s>\n",
      "Generated:  I prefer cold water so i would like to one day surf in the ocean as well. \n",
      "Grammar score: 0.9825499057769775\n",
      "Truth:  The Green Lantern film was originally going to be a comedy starring Jack Black. I would like to have seen that version. It would probably be better than the actual one. Jack is a natural comedian. I wonder what Bill Murray would have said.</s>\n",
      "Generated:  Oh, you would know more about this game than I! It seems to be quite popular. Do you by chance have any other recommendations for movies? My sister has Netflix and she keeps asking for my input. She likes all genres except horror.\n",
      "Grammar score: 0.013566430658102036\n",
      "Truth:  Cobb had to remind her that it was all a dream, that was one heck of a wake up call!</s>\n",
      "Generated:  Yea, I am not sure if I would like how it ended though\n",
      "Grammar score: 0.9530771970748901\n",
      "Truth:  I'd like to do it, of course. But I just can't. I'm afraid I'd fall down in the middle of it.</s>\n",
      "Generated:  I would like to do it, naturally. But I just can't. I'm afraid I wouldn't make it through all right.\n",
      "Grammar score: 0.9882885217666626\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  No, but it fits with his persona and his famous moonwalk, which I cannot do, even though I'd like to! I thought it was interesting that the word robot comes from a Czech word that means 'forced labor'. Puts a different spin on us using machines.</s>\n",
      "Generated:  No, but it fits with his persona and his famous MoonWalk, which I would still like to see live, if he were alive today\n",
      "Grammar score: 0.6257501244544983\n",
      "Truth:  It is.  I'd like to think mine turned out okay.  Really, it was easy as long as you're able to respond to the demands of my kids.</s>\n",
      "Generated:  it is.  i would like to think mine turned out okay\n",
      "Grammar score: 0.9960592985153198\n",
      "Truth:  I can't remember, but I've got the receipt somewhere.Would you like to see it?</s>\n",
      "Generated:  It cost me $300.I would have liked it more if it had been cheaper.\n",
      "Grammar score: 0.43083661794662476\n",
      "Truth:  I'd like to. But Bob, are you sure it will be all right? I would hate you to make your boss angry or even lose your job.</s>\n",
      "Generated:  Sure. Where would you like to eat?\n",
      "Grammar score: 0.3630974292755127\n",
      "Truth:  I'm a big fan of animated movies though, so I would like to watch this one. How about you?</s>\n",
      "Generated:  There is also a TV show based on the same characters\n",
      "Grammar score: 0.004055940080434084\n",
      "Truth:  Yeah, sure. \"shark barking\" I would prefer a dog. Easier to pet, easier to take to the vet. Dogs are pretty smart.</s>\n",
      "Generated:  I would like to have a pet dragon. Or a phoenix.\n",
      "Grammar score: 0.9947313070297241\n",
      "EPOCH ____________________________________________________________\n",
      "Truth:  I would like to, maybe I'll check to see if it's available streaming online. tell me more about the drama.</s>\n",
      "Generated:  I would like to watch this movie maybe I'll check to see if its available streaming online\n",
      "Grammar score: 0.9949823021888733\n",
      "Truth:  how interesting. i would like to know more about factories</s>\n",
      "Generated:  how interesting. i would like to know more about factories\n",
      "Grammar score: 0.9948545694351196\n",
      "Truth:  I do recall hearing it was quite powerful. I'd like to visit Osaka one day, what about you? Tokyo perhaps?</s>\n",
      "Generated:  I do recall hearing it was quite powerful. Maybe one day I can travel with my sister who has never been outside the US. She would love Tokyo Disneyland!\n",
      "Grammar score: 0.015324964188039303\n",
      "Truth:  Then I would like to find a general doctor.</s>\n",
      "Generated:  I would like to see a cardiologist because my father had a heart attack when he was young.\n",
      "Grammar score: 0.9878225922584534\n",
      "Truth:  I would like one of those, lol. Interestingly enough, Darrell Green would stuff tootsie rolls in his socks before games, claiming they aided his speed.</s>\n",
      "Generated:  Yeah, it's amazing how people come up with new ideas to make learning fun and effective. It's also funny how some people would still prefer traditional classrooms. They are probably against anything unusual or different. Personally, I'd like my education as diverse as possible.\n",
      "Grammar score: 0.9907172322273254\n",
      "Truth:  What about coming back? I'd like to come back at about 7 p. m.</s>\n",
      "Generated:  OK. I'd like to book a ticket for this one.\n",
      "Grammar score: 0.996282160282135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.679188768863678, metrics={'train_runtime': 361.8651, 'train_samples_per_second': 5.527, 'train_steps_per_second': 0.691, 'total_flos': 1.475172446611968e+16, 'train_loss': 0.679188768863678, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5250aaa3-02de-4794-ba53-19d3e0c233d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/dglandorf/gctg/lib64/python3.11/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647989a0-c095-48eb-bb71-52f1b52a091a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gctg)",
   "language": "python",
   "name": "gctg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

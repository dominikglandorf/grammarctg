{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15365f89-66d7-45e1-ad97-dc4610aeeeb5",
   "metadata": {},
   "source": [
    "# Exp018: Conditional instruction fine-tuning\n",
    "This experiment aims at instruction fine-tuning from existing skills in the dataset to train the model on single constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c19775e-4db4-4d58-9310-0e82d15f32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 17:15:53.169404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "\n",
    "import pickle\n",
    "from torch.utils.data import RandomSampler, Subset\n",
    "import numpy as np\n",
    "import json\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(f'../source')\n",
    "import helpers\n",
    "import models\n",
    "import importlib\n",
    "#importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d860d9f7-cfd7-4c60-8ab7-ac59fa12d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "out_file = '../data/corpus_classification_all.pkl'\n",
    "preprossed_dataset_file = '../data/SFT_data.jsonl'\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "nrs = [616] #[58, 616]#\n",
    "classifier = models.load_classifier(616, \"corpus_training\")\n",
    "EOP = \"[/INST]\"\n",
    "egp = helpers.get_egp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bf8eb-e0b0-4d72-92de-39d59362eafa",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f860be6b-ba7d-498f-9147-f65d5d8b1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_file, 'rb') as f:\n",
    "    all_hit_indices = pickle.load(f)\n",
    "    all_hit_sentences = pickle.load(f)\n",
    "    extracts = pickle.load(f)\n",
    "\n",
    "data = [{\"context\": extracts[idx][0], \"response\": extracts[idx][1], \"nr\": nr} for nr in nrs for idx in all_hit_indices[nr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58eebd3e-f3da-4d9d-89c4-92bccf50d23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7571fd99ec04da9bf843bb744c970ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    rules = egp[egp['#'].isin(example['nr'] if type(example['nr']) == list else [example['nr']])]\n",
    "    constraints = os.linesep.join(\"- \" + rules['SubCategory'] + \": \" + rules['Can-do statement']) # \" - \" + rules['guideword']\n",
    "    context = os.linesep.join([(\"A\" if (i%2==0) else \"B\") + \": \" + utt for i, utt in enumerate(example[\"context\"])])\n",
    "\n",
    "    instruction = f\"\"\"Write the response of A and include this grammatical items in the response.\n",
    "{constraints}\"\"\"\n",
    "   # instruction = 'Write an answer of A that includes the affirmative form of \"would like\".'\n",
    "    \n",
    "    prompt_completion = f\"\"\"[INST] \n",
    "{instruction}\n",
    "Dialog:\n",
    "{context} {EOP} \n",
    "A: {example['response']}</s>\"\"\"\n",
    "    \n",
    "    return prompt_completion, prompt_completion.index(EOP)+len(EOP)\n",
    "    \n",
    "with open(preprossed_dataset_file, 'w') as f:\n",
    "    for item in tqdm(data):\n",
    "        # line['prompt'], line['completion'] = formatting_func(item) # for completion chat format\n",
    "        item['text'], item['prompt_len']  = formatting_func(item)\n",
    "        #print(item)\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2f6bc-a141-4832-8f94-af0732f0b8da",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb69f077-228e-4054-95a7-9bd8c359bd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4bd6245ada4df9a093e7415a8217b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files=preprossed_dataset_file, split='train', cache_dir=os.getenv('CACHE_DIR'))\n",
    "train_test_split = dataset.train_test_split(test_size=0.05)\n",
    "train_dataset, test_dataset = train_test_split['train'], train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f055c75-4e39-4153-bbd6-66e03673461e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['I need to transfer money.',\n",
       "  'Do you know which account you want to take the money from?',\n",
       "  'From my savings account.',\n",
       "  'Where are you transferring the money to?'],\n",
       " 'response': 'I would like it transferred to my checking account.',\n",
       " 'nr': 616,\n",
       " 'text': \"[INST] \\nWrite the response of A and include this grammatical items in the response.\\n- would: Can use the affirmative form with 'like'. \\nDialog:\\nA: I need to transfer money.\\nB: Do you know which account you want to take the money from?\\nA: From my savings account.\\nB: Where are you transferring the money to? [/INST] \\nA: I would like it transferred to my checking account.</s>\",\n",
       " 'prompt_len': 314}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e897370-34b9-4776-9d6c-5eaa66f24db6",
   "metadata": {},
   "source": [
    "## Load and prepare base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b070943-63d9-418e-ac09-d7515602f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b489244c-bcab-4da7-ad02-abbce1048030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d45f9551e10496ebe615c4185b376eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, cache_dir=os.getenv('CACHE_DIR'), device_map=\"auto\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.getenv('CACHE_DIR'), device_map=\"auto\")\n",
    "model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, cache_dir=os.getenv('CACHE_DIR'), padding_side=\"right\")\n",
    "#tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302a546-0eff-452a-b195-dbe6ceede8a9",
   "metadata": {},
   "source": [
    "### Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a331450b-9ac4-4a40-9e2a-2758706a076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] \n",
      "Write the response of A and include this grammatical items in the response.\n",
      "- would: Can use the affirmative form with 'like'. \n",
      "Dialog:\n",
      "A: I do too! Do you enjoy fiction?  It uses imagination!\n",
      "B: Yes I love reading but cannot do speed reading like Anne Jones - 4700 words a minute!\n",
      "A: Yes, I know she is unreal! Do you know how they measure that? They don't have to understand what they're reading?\n",
      "B: I think they do understand it!  But at least there is no moral panic these days about reading like there was in the 18th century! [/INST] \n",
      "A: I agree! I also enjoy fiction and appreciate the use of imagination it entails. I was just wondering how they measure someone's reading speed like Anne Jones'. I guess they don't necessarily have to understand what they're reading to achieve such high numbers.\n",
      "\n",
      "B: Yes, I love reading as well, but I can't read as fast as Anne Jones. I believe they do understand what they're reading, even if they're skimming through it quickly. And you're right, there's no longer the same moral panic about reading that there was in the 18th century.\n"
     ]
    }
   ],
   "source": [
    "example = random.choice(test_dataset)\n",
    "#example = train_dataset[10]\n",
    "#example['nr'] = [58, 616]\n",
    "#example['text'] = formatting_func(example)\n",
    "#print(example['text'])\n",
    "\n",
    "#converted_sample = [\n",
    "#    {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "#    #{\"role\": \"assistant\", \"content\": example[\"completion\"]},\n",
    "#]\n",
    "#model_input = tokenizer.apply_chat_template(converted_sample, return_tensors=\"pt\").to(device)\n",
    "\n",
    "eval_prompt = example['text'][:example['prompt_len']+4]\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    token_ids = model.generate(**model_input, max_new_tokens=1024, pad_token_id=32000)[0]\n",
    "    output_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f788de-4d48-497b-99fd-add052918ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds, verbose=False, num_samples=25, datasets={\"train\": train_dataset, \"test\": test_dataset}):\n",
    "    if verbose: print(\"EPOCH\", \"___\" * 20)\n",
    "    all_scores = {}\n",
    "    for name, ds in datasets.items():\n",
    "        random_sampler = RandomSampler(ds, num_samples=num_samples)\n",
    "        subset = ds[random_sampler]\n",
    "        prompts = [text[:prompt_len+4] for text, prompt_len in zip(subset['text'], subset['prompt_len'])]\n",
    "        if verbose: print(prompts)\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        model_input = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        tokenizer.padding_side = \"right\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            token_ids = model.generate(**model_input, max_new_tokens=128, pad_token_id=32000)\n",
    "            outputs = tokenizer.batch_decode(token_ids[:,model_input['input_ids'].shape[1]:], skip_special_tokens=True, device=\"cpu\")\n",
    "        scores = models.probe_model(classifier, outputs)[0]>0.5\n",
    "        all_scores[name] = scores\n",
    "        \n",
    "        if verbose:\n",
    "            truths = [text[prompt_len+4:-4] for text, prompt_len in zip(subset['text'], subset['prompt_len'])]\n",
    "            for truth, output in zip(truths, outputs):\n",
    "                print(f\"Truth: {truth}\")\n",
    "                print(f\"Gener: {output}\")\n",
    "            print(f\"Grammar detected: {scores}\")\n",
    "        print(list(zip(outputs,scores))[:10])\n",
    "        \n",
    "    return {f\"success_{name}\": all_scores[name].float().mean().item() for name in datasets.keys()}\n",
    "\n",
    "#compute_metrics([], verbose=False, datasets={\"test\": test_dataset}) # test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b8ed6-4a6a-4237-acc5-e0633d076a4b",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125626b6-a4ab-41eb-8940-fa8fa0745d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    #modules_to_save=[\"embeddings\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b8ee436-02f3-47b9-b605-830bff043f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../models/mistral_FT\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    #save_steps=25,\n",
    "    logging_steps=5,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"gctg\",\n",
    "    #load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    per_device_eval_batch_size=8,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa48307e-a483-404f-bb12-170ef022ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d25eac7997d45c5827034a0852c1960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#eval_dataset = dataset.train_test_split(test_size=0.001)\n",
    "#eval_dataset = eval_dataset[\"test\"]\n",
    "collator = DataCollatorForCompletionOnlyLM(\"[/INST]\", tokenizer=tokenizer)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    data_collator=collator,\n",
    "    #preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    "    #neftune_noise_alpha=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "255a8ea4-8027-4101-8a7e-fbfa620066d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/418 04:02 < 05:24, 0.73 it/s, Epoch 0.43/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Success Train</th>\n",
       "      <th>Success Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.225700</td>\n",
       "      <td>1.823330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.238300</td>\n",
       "      <td>1.762281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.259700</td>\n",
       "      <td>1.721290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"I'd like to try it.  Do you know where I can find a court?\", tensor(True)), (\"I would like to be a student of Jon Hamm's. I would like to know what he would be like as a teacher.\", tensor(True)), ('I would like a kitchen that is warm and welcoming.', tensor(True)), ('I would like to go to the beach more often.', tensor(True)), ('I would like to see the first video game. I would like to see the first video game.', tensor(True)), ('I would like to go to Japan and visit the temples and shrines. I would like to see the cherry blossoms in Japan', tensor(True)), ('I would like to go to the South by Southwest festival', tensor(True)), (\"I'd like to go to Shanghai. I'd like to see the Bund and the Oriental Pearl Tower.\", tensor(True)), (\"I would like to.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I'd like to read more of it.  I\", tensor(True)), (\"I'd like to try some Chinese food, too. I'd like to know what it tastes like.\", tensor(True))]\n",
      "[('I would like to have a car. I would like to be able to go anywhere I want to go.', tensor(True)), (\"I'd like to see a proposal that includes a path to citizenship for the 1.8 million dreamers.  I'd like to see a proposal that includes a path to citizenship for the 11 million undocumented immigrants.  I'd like to see a proposal that includes a path to citizenship for the 1.8 million TPS recipients.  I'd like to see a proposal that includes a path to citizenship for the 1.5 million farm workers.  I'd like to see a proposal that includes a path to citizenship for the 1.5\", tensor(True)), ('I would like to see him dance ballet. I would like to see him rap too.', tensor(True)), (\"I'd like to try the salty ones.\", tensor(True)), ('I would like to go to a nightclub.', tensor(True)), ('I would like to suggest the Honda CR-V.', tensor(True)), ('I would like a cup of coffee.', tensor(True)), ('I would like to know that too, I would like to know how many cats are in the world', tensor(True)), ('Yes, they are supposed to hide pots of gold at the end of rainbows.', tensor(False)), ('I would like to invite you to go with me to the concert tonight.', tensor(True))]\n",
      "[('I would like to see that, I would like to see the hummingbirds in action.  I would like to see the spiders in action too.', tensor(True)), (\"I would like to read some of them too, I'm sure they're great.\", tensor(True)), ('I would like to see it.', tensor(True)), ('I would like to check in on the 15th.', tensor(True)), ('I would like to see more information on the cost of this project. I would like to know how much it would cost to send a human to Mars and how much it would cost to send a robot.', tensor(True)), ('I would like to go there someday.', tensor(True)), (\"I'd like to have the same dish as you.\", tensor(True)), (\"I would like to play basketball but I don't have the time.\", tensor(True)), ('I would like to see that. I would like to see a bus that goes 300 mph', tensor(True)), ('I would like to read more about that. I would like to read a poem that is 1000 pages long.', tensor(True))]\n",
      "[('I would like to see a compromise that includes border security and a path to citizenship for the dreamers', tensor(True)), ('I would like to know more about that, I would like to know if they have any idea about the aliens', tensor(True)), (\"I'd like to visit the temples and shrines, and eat some sushi.\", tensor(True)), ('I would like to see that, I would like to see a dog and elephant interacting', tensor(True)), (\"I'd like to book a ticket for that flight.\", tensor(True)), (\"I'd like to try the salty ones.\", tensor(True)), ('I would like to get rid of my phone, but I need it for work.', tensor(True)), (\"I'd like to, but I don't have a webcam. \", tensor(True)), (\"I'd like to. I'd like to have some more fish, too.\", tensor(True)), ('Yes, I think so. I would like to see more clothes that are practical and comfortable.', tensor(True))]\n",
      "[('I would like to open a business checking account. I would like to have checks and a debit card.', tensor(True)), ('I know what you mean. I would like to be able to live without the internet. I think I would be more productive.', tensor(True)), ('I would like to hike up there and see the view from the top', tensor(True)), ('I would like to see the movie \"The Secret Life of Bees\" because it is based on a book that I read and I think it would be interesting to see it on the big screen.', tensor(True)), (\"I'd like to hear it before I make up my mind.\", tensor(True)), (\"That's fine. I'd like to take you to some interesting places.\", tensor(True)), ('I would like to listen to that. I wonder if he was the first to record a song.', tensor(True)), ('I would like to see it. I think I would like it. I am a fan of movies based on real stories.', tensor(True)), (\"I'd like to see how the movie portrays the story. I'd like to see how the movie portrays the story.\", tensor(True)), (\"I'd like to know too. I'd like to hear it.\", tensor(True))]\n",
      "[(\"I would like to know more about the Earth's atmosphere. I know that it is divided into 5 layers. \", tensor(True)), (\"I'd like to go there sometime. I'd like to go hiking in the mountains.\", tensor(True)), ('I would like to see it.', tensor(True)), (\"I'd like to try the salty ones. I'm not a big fan of sweet things.\", tensor(True)), (\"I'd like to reserve the room for Saturday, March 15th.\", tensor(True)), (\"I'd like to live in a studio apartment again.  I think it would be fun.  I'd like to live in a studio apartment in a high-rise building.  I think it would be fun to live in a high-rise building.  I'd like to live in a high-rise building in a big city.  I think it would be fun to live in a high-rise building in a big city.  I'd like to live in a high-rise building in a big city with a view of the ocean.  I think it would be fun to live in a high\", tensor(True)), (\"I'd like to cash a check for $ 100.\", tensor(True)), (\"I'd like to see it.\", tensor(True)), ('Yes, they are! I would like to find one!', tensor(True)), (\"I would like to hear him more. I think I would like him. I would like to hear the show that is on 105.3 in Chicago. I think it's called the Score.\", tensor(True))]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/trl/trainer/sft_trainer.py:360\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 360\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/accelerate/accelerator.py:2001\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2001\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5250aaa3-02de-4794-ba53-19d3e0c233d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/dglandorf/gctg/lib64/python3.11/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "#trainer.save_model(\"../models/mistral_FT_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "647989a0-c095-48eb-bb71-52f1b52a091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, cache_dir=os.getenv('CACHE_DIR'), device_map=\"auto\")\n",
    "#model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, cache_dir=os.getenv('CACHE_DIR'), padding_side=\"right\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, \"../models/mistral_FT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fb416-b4c7-4719-94c7-e8be23b7f673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gctg)",
   "language": "python",
   "name": "gctg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
